{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Web Traffic Forecasting\n",
    "Ruoxin Jiang and Bingyan Hu\n",
    "## Overview\n",
    "___\n",
    "### Task\n",
    "The goal of our project is to forecast web traffic time series for online pages. Forecasting time series is challenging since we need to combine its seasonality, trend and other factors intelligently in modeling; the historical data itself is insufficient to capture uncertainty in future events. \n",
    "\n",
    "We present a hierchical time series forecasting model using Edward and demostrate three rounds of Box's loop below.\n",
    "\n",
    "+ Hypothesis\n",
    "\n",
    "### Data Source\n",
    "We obtain real time series data from [a recent Kaggle competition](https://www.kaggle.com/c/web-traffic-time-series-forecasting). Each time series represents daily page views of a particular Wikipedia article from **07/01/2015** to **09/10/2017**. \n",
    "\n",
    "The model is trained on data before **07/10/2017** and we forecast number of page visits in last 60 days from **07/10/2017** to **09/10/2017**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from pipeline import *\n",
    "from cross_validation import cross_validation\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (24, 12)\n",
    "matplotlib.rcParams['lines.linewidth'] = 2\n",
    "matplotlib.rcParams['xtick.labelsize'] = 18\n",
    "matplotlib.rcParams['ytick.labelsize'] = 18\n",
    "matplotlib.rcParams['xtick.color'] = 'w'\n",
    "\n",
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Round 1: \n",
    "___\n",
    "### 1.Data\n",
    "We randomly pick a wikipedia article data with `ds` and `views` in long format.\n",
    "\n",
    "+ selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into DataFrame\n",
    "FPATH = \"./data/nfl_teams.csv\"\n",
    "pages = [\"Atlanta_Falcons_en.wikipedia.org_mobile-web_all-agents\", \n",
    "         \"Dallas_Cowboys_en.wikipedia.org_mobile-web_all-agents\"]\n",
    "\n",
    "timeseries = get_timeseries(FPATH)\n",
    "ts_dfs = []\n",
    "for p in pages:\n",
    "    print(\"Preparing timeseries %s\" % p)\n",
    "    df = setup_dataframe(timeseries[p])\n",
    "    ts_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split history (train) and future (test)\n",
    "sdate = pd.datetime(2017, 7, 10)\n",
    "ts_data = []\n",
    "for df in ts_dfs: \n",
    "    history, future, y_scale = split_train_test(df, sdate)\n",
    "    ts_data.append({\n",
    "        \"history\": history, \"future\": future, \"y_scale\": y_scale\n",
    "    })\n",
    "    \n",
    "print(\"Extracting features\")\n",
    "ts = ts_data[0] # same feature matrix for all test series  \n",
    "train_data = extract_features(ts[\"history\"])\n",
    "test_data = extract_features(ts[\"future\"], changepoints_t=train_data[\"t_change\"])\n",
    "assert(all(train_data[\"X\"].columns ==  test_data[\"X\"].columns))\n",
    "assert(all(train_data[\"t_change\"] == test_data[\"t_change\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: FB prophet regression model\n",
    "We build a regression model similar to [Facebook Prophet](https://peerj.com/preprints/3190/); it combines trend, seasonality and holiday components with non-linear smoothers applied to regressors $t \\in \\mathbb{Z}^{T}$. \n",
    "\n",
    "$$y(t) = g(t) + s(t) + h(t) + \\epsilon_{t}  $$\n",
    "\n",
    "- **Trend** <br/> $$g(t) = (k + \\mathbf{a}(t)^{T} \\boldsymbol{\\delta})t + (m + \\mathbf{a}(t)^{T} \\boldsymbol{\\gamma})$$\n",
    "    - $k$ is the growth rate (slope)\n",
    "    - $m$ is the offset (intercept)\n",
    "    - $S$ changepoints are explicitly defined to allow trend changes at times $s_{j \\in {1,2,...,S}}$\n",
    "        - $\\mathbf{a}(t) \\in \\{0,1\\}^{S}$ are changepoint indicators\n",
    "        - $\\delta_{j} \\sim Laplace(0,\\tau)$ is the change of rate at time $s_{j}$\n",
    "        - $\\gamma_{j}$ is set to $-s_{j}\\delta_{j}$ to make the function continuous</br>\n",
    "\n",
    "\n",
    "- **Seasonality** <br/>\n",
    "We construct Fourier series to approximate periodic seaonality.\n",
    "$$s(t) = \\sum_{n=1}^{N}(a_{n}cos(\\frac{2\\pi nt}{P}) + b_{n}sin(\\frac{2\\pi nt}{P} )) = X(t)  \\boldsymbol{\\beta}$$\n",
    "    - $\\boldsymbol{\\beta} = [a_{1}, b_{1} , ... , a_{N}, b_{N}]^{T}$ and $\\boldsymbol{\\beta} \\sim Normal(0,\\sigma^{2})$\n",
    "    - yearly -> (P = 365.25, N = 10)\n",
    "    - weekly -> (P = 7, N = 3)\n",
    "    \n",
    "\n",
    "- **Holiday/Events** <br/>\n",
    "Assuming holidays are independnet, we assign each holiday with a parameter $\\kappa_{i}$\n",
    "$$h(t) = Z(t) \\boldsymbol{\\kappa}$$\n",
    "    - $Z(t) = [\\boldsymbol{1}(t\\in D_{1}) , ... , \\boldsymbol{1}(t\\in D_{L})]$\n",
    "    - $\\boldsymbol{\\kappa} \\sim Normal(0, \\nu^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before modeling, we transform the raw data and extract features into proper format. The input data includes\n",
    "\n",
    "- X\n",
    "    - **t: ** time index\n",
    "    - **X: ** seasonality vector after fourier transformation\n",
    "    - **A: ** changepoint vector given time and number of change points\n",
    "    - **sigmas: ** fixed scale on seasonality priors\n",
    "- y\n",
    "    - **y_scaled: ** `maxdiff(log(views))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TS = len(ts_data)\n",
    "S = len(train_data[\"t_change\"])\n",
    "K = train_data[\"X\"].shape[1]\n",
    "\n",
    "t = tf.placeholder(tf.float32, shape=None, name=\"t\")              # time index\n",
    "A = tf.placeholder(tf.float32, shape=(None, S), name=\"A\")         # changepoint indicators\n",
    "t_change = tf.placeholder(tf.float32, shape=(S), name=\"t_change\") # changepoints_t\n",
    "X = tf.placeholder(tf.float32, shape=(None, K), name=\"X\")         # season vectors\n",
    "sigmas = tf.placeholder(tf.float32, shape=(K,), name=\"sigmas\")    # scale on seasonality prior\n",
    "tau = tf.placeholder(tf.float32, shape=(), name=\"tau\")\n",
    "\n",
    "k = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))     # initial slope\n",
    "m = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))     # initial intercept\n",
    "\n",
    "sigma_obs = Normal(loc=tf.zeros(1), scale=0.5*tf.ones(1))   # noise\n",
    "delta = Laplace(loc=tf.zeros(S), scale=tau*tf.ones(S))    # changepoint rate adjustment\n",
    "gamma = tf.multiply(-t_change, delta)\n",
    "beta = Normal(loc=tf.zeros(K), scale=sigmas*tf.ones(K))      # seasonal\n",
    "trend_loc = (k + ed.dot(A, delta)) * t + \\\n",
    "            (m + ed.dot(A, gamma))\n",
    "seas_loc = ed.dot(X, beta)\n",
    "y = Normal(loc = trend_loc + seas_loc, scale = sigma_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Inference: HMC\n",
    "Given train data, the goal is to infer $k,m,\\boldsymbol{\\delta},\\tau,\\boldsymbol{\\beta}$ and $\\sigma$, where k and m are trend model parameters, $\\boldsymbol{\\delta}$ are latent variables for rate adjustment, $\\boldsymbol{\\beta}$ are smoothers for seasonality, $\\tau$ and $\\sigma$ are variance component parameters.\n",
    "\n",
    "In this analysis, we use Monte Carlo with `ed.HMC` to infer all the latent variables. All training data are passed in for inference and we tune step_size $= 0.0005$, n_steps $= 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITR = 5000\n",
    "kinit, minit = init_km(ts_data[0][\"history\"])\n",
    "print(\"[+] Initial slope / intercept: %f, %f\" % (kinit, minit))\n",
    "qk = Empirical(params=tf.Variable(kinit * tf.ones([ITR, 1])))\n",
    "qm = Empirical(params=tf.Variable(minit * tf.ones([ITR, 1])))\n",
    "qsigma_obs = Empirical(params=tf.Variable(tf.ones([ITR, 1])))\n",
    "qbeta = Empirical(params=tf.Variable(tf.zeros([ITR, K])))\n",
    "qdelta = Empirical(params=tf.Variable(tf.zeros([ITR, S])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_dict = {\n",
    "    k: qk, m: qm, sigma_obs: qsigma_obs,\n",
    "    beta: qbeta, delta: qdelta}\n",
    "\n",
    "data_dict = {\n",
    "    y: ts_data[0][\"history\"][\"y_scaled\"].as_matrix(),\n",
    "    t: train_data[\"t\"],\n",
    "    X: train_data[\"X\"],\n",
    "    sigmas: train_data[\"sigmas\"],\n",
    "    A: train_data[\"A\"],\n",
    "    t_change: train_data[\"t_change\"],\n",
    "    tau: 0.05,\n",
    "}\n",
    "\n",
    "inference = ed.HMC(posts_dict, data=data_dict)\n",
    "STEP_SIZE = 5e-4\n",
    "N_STEPS = 2\n",
    "inference.run(step_size=STEP_SIZE, n_steps=N_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction import *\n",
    "nburn = int(ITR / 2)\n",
    "stride = 10\n",
    "sess = ed.get_session()\n",
    "post_params = {\n",
    "    \"k\": qk.params.eval()[nburn:ITR:stride],\n",
    "    \"m\": qm.params.eval()[nburn:ITR:stride],\n",
    "    \"beta\": qbeta.params.eval()[nburn:ITR:stride],\n",
    "    \"delta\": qdelta.params.eval()[nburn:ITR:stride]\n",
    "}\n",
    "ts = ts_data[0]\n",
    "pred_df =  make_future_dataframe(ts[\"history\"], ts[\"future\"].shape[0])\n",
    "pred_df = predict_fixed(pred_df, post_params, test_data)\n",
    "y_true = ts[\"future\"][\"y_scaled\"].as_matrix()\n",
    "_  = evaluate(y_true, pred_df[\"y\"])\n",
    "\n",
    "plt.plot(ts[\"future\"][\"ds\"], ts[\"future\"][\"y_scaled\"], lw=4, color='b')\n",
    "plt.plot(ts[\"future\"][\"ds\"], pred_df[\"y\"], color='r')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "            \n",
    "# test_data_dict =  {\n",
    "#     t: test_data[\"t\"],\n",
    "#     X: test_data[\"X\"],\n",
    "#     sigmas: test_data[\"sigmas\"],\n",
    "#     A: test_data[\"A\"],\n",
    "#     t_change: test_data[\"t_change\"],\n",
    "#     tau: 0.05,\n",
    "# }\n",
    "# CI = [20, 80]\n",
    "# y_true = ts_data[0][\"future\"][\"y_scaled\"]\n",
    "# y_pred = predict(y, posts_dict, test_data_dict, SAMPLE=500)\n",
    "# y_pred_mean = np.mean(y_pred, axis=0)[0]\n",
    "# y_pred_lower = np.percentile(y_pred, CI, axis=0)[0][0]\n",
    "# y_pred_upper = np.percentile(y_pred, CI, axis=0)[1][0]\n",
    "# plt.plot(ts_data[0][\"future\"][\"t\"], y_true,lw=4, color='b')\n",
    "# plt.plot(ts_data[0][\"future\"][\"t\"], y_pred_mean,lw=4, color='r')\n",
    "# plt.fill_between(ts_data[0][\"future\"][\"t\"], y_pred_lower,y_pred_upper,color='b',alpha=.05)\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(ts_data[0][\"future\"][\"t\"],  y_pred_mean - y_true, color='b', label=\"residual\")\n",
    "# plt.plot(ts_data[0][\"future\"][\"t\"],  np.zeros(y_true.shape), label=\"residual\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(411) \n",
    "plt.plot(ts[\"future\"][\"ds\"], pred_df[\"yearly\"])\n",
    "plt.subplot(412)\n",
    "plt.plot(ts[\"future\"][\"ds\"], pred_df[\"weekly\"])\n",
    "plt.subplot(413)\n",
    "plt.plot(ts[\"future\"][\"ds\"], pred_df[\"trend\"])\n",
    "plt.subplot(414)\n",
    "plt.plot(ts[\"future\"][\"ds\"], y_true - pred_df[\"y\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "pred_df.index = pred_df[\"ds\"]\n",
    "decomposition = seasonal_decompose(y_true - pred_df[\"y\"])\n",
    "plt.subplot(411) \n",
    "plt.plot(decomposition.trend)\n",
    "plt.subplot(412)\n",
    "plt.plot(decomposition.seasonal)\n",
    "plt.subplot(413)\n",
    "plt.plot(decomposition.resid)\n",
    "plt.subplot(414)\n",
    "plt.plot(ts[\"future\"][\"ds\"], y_true - pred_df[\"y\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "history_pred_df = pd.DataFrame({\"ds\": ts[\"history\"][\"ds\"].copy(),\n",
    "                                \"t\": ts[\"history\"][\"t\"].copy()})\n",
    "history_pred_df.reset_index(inplace=True, drop=True)\n",
    "history_pred_df = predict_fixed(history_pred_df, post_params, train_data)\n",
    "pred_df.head()\n",
    "plt.subplot(411) \n",
    "plt.plot(ts[\"history\"][\"ds\"], history_pred_df[\"yearly\"])\n",
    "plt.subplot(412)\n",
    "plt.plot(ts[\"history\"][\"ds\"], history_pred_df[\"weekly\"])\n",
    "plt.subplot(413)\n",
    "plt.plot(ts[\"history\"][\"ds\"], history_pred_df[\"trend\"])\n",
    "plt.subplot(414)\n",
    "plt.plot(ts[\"history\"][\"ds\"], ts[\"history\"][\"y_scaled\"].as_matrix() - history_pred_df[\"y\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Criticism\n",
    "- Visualization of the residuals\n",
    "- Pointwise Evaluation\n",
    "    - MAPE\n",
    "    - SMAPE\n",
    "    - MSE\n",
    "- PPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_true, pred_df[\"y\"])\n",
    "sns.distplot(pred_df[\"y\"] - y_true)\n",
    "plt.show()\n",
    "sns.distplot((pred_df[\"y\"] - y_true)/y_true)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior check\n",
    "sess = ed.get_session()\n",
    "kmean, kstddev = sess.run([qk.mean(), qk.stddev()])\n",
    "print(\"Inferred posterior k: mean = %f, stddev = %f\" % (kmean, kstddev))\n",
    "mmean, mstddev = sess.run([qm.mean(), qm.stddev()])\n",
    "print(\"Inferred posterior m: mean = %f, stddev = %f\" % (mmean, mstddev))\n",
    "\n",
    "nburn = int(ITR / 2)\n",
    "stride = 10\n",
    "sns.distplot(qk.params.eval()[nburn:ITR:stride])\n",
    "plt.show()\n",
    "sns.distplot(qm.params.eval()[nburn:ITR:stride])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_post = ed.copy(y, posts_dict)\n",
    "ty_rep, ty = ed.ppc(lambda xs, zs: tf.reduce_max(tf.cast(xs[y_post], tf.float32)), \n",
    "       data={y_post:ts_data[0][\"history\"][\"y_scaled\"].as_matrix(), \n",
    "            t: train_data['t'],\n",
    "            A: train_data['A'], \n",
    "            X: train_data['X'].as_matrix(), \n",
    "            sigmas: train_data['sigmas'], \n",
    "            t_change: train_data[\"t_change\"]},n_samples=500)    \n",
    "\n",
    "# y: ts_data[0][\"history\"][\"y_scaled\"].as_matrix(),\n",
    "#     t: train_data[\"t\"],\n",
    "#     X: train_data[\"X\"],\n",
    "#     sigmas: train_data[\"sigmas\"],\n",
    "#     A: train_data[\"A\"],\n",
    "#     t_change: train_data[\"t_change\"]\n",
    "\n",
    "ed.ppc_stat_hist_plot(\n",
    "    ty[0], ty_rep, stat_name=r'$T \\equiv$max', bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Round 1.5: \n",
    "___\n",
    "\n",
    "### Improvements\n",
    "- motify init k and m\n",
    "- remove holiday features why us public holiday not works, require in-domain knowledge\n",
    "- tau from fixed value to latent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 2: \n",
    "___\n",
    "### 2.Model\n",
    "We modify the model by creating global latent variable inferring local seasonality  \n",
    "- Local latend variable\n",
    "    - \n",
    "- Global latent variable\n",
    "    - `gbeta`\n",
    "    \n",
    "Given train data, the goal is to infer $k,m,\\boldsymbol{\\delta},\\tau,\\boldsymbol{\\beta}$ and $\\sigma$, where k and m are trend model parameters, $\\boldsymbol{\\delta}$ are latent variables for rate adjustment, $\\boldsymbol{\\beta}$ are smoothers for seasonality, $\\tau$ and $\\sigma$ are variance component parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_results(ts_data, predictions, metrics):\n",
    "    for i, df in enumerate(ts_dfs):\n",
    "        plt.plot(ts_data[i][\"future\"][\"ds\"], ts_data[i][\"future\"][\"y_scaled\"])\n",
    "        plt.plot(predictions[i][\"ds\"], predictions[i][\"y_scaled_pred\"], '#2ca02c')\n",
    "        plt.show()  \n",
    "    m_pd = pd.DataFrame.from_dict(metrics)\n",
    "    m_pd.loc['mean'] = m_pd.mean()\n",
    "    print(m_pd)\n",
    "    print()\n",
    "\n",
    "def visualize_cross_validation(ts_dfs, predictions, metrics):\n",
    "    for i, df in enumerate(ts_dfs):\n",
    "        df = df[df[\"ds\"] > pd.datetime(2016,6,1)]\n",
    "        plt.plot(df[\"ds\"], df[\"y\"])\n",
    "        for pred in predictions:\n",
    "            plt.plot(pred[i][\"ds\"], pred[i][\"y_pred\"], '#2ca02c')\n",
    "        plt.show()\n",
    "    \n",
    "    metrics_df = pd.DataFrame(columns=['start', 'end', 'MAPE_avg', 'SMAPE_avg'])\n",
    "    for i, m_cutoff in enumerate(metrics):\n",
    "        dmin, dmax = predictions[i][0][\"ds\"].min(), predictions[i][0][\"ds\"].max()\n",
    "        avg_mape_scaled = np.mean([m[\"MAPE\"] for m in m_cutoff])\n",
    "        avg_smape_scaled = np.mean([m[\"SMAPE\"] for m in m_cutoff])\n",
    "        metrics_df = metrics_df.append({\"start\": dmin,\n",
    "                                        \"end\": dmax,\n",
    "                                        \"MAPE_avg\": avg_mape_scaled, \n",
    "                                        \"SMAPE_avg\": avg_smape_scaled}, ignore_index=True)\n",
    "    \n",
    "    print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "results = []\n",
    "models_test = [Model1(), Model3()]\n",
    "print(train_data.keys())\n",
    "for model in models_test:\n",
    "    p, m = pipeline(ts_data, model, train_data, test_data, \n",
    "                    ITR=5000, N_STEPS=2, STEP_SIZE=5e-4)\n",
    "    results.append({\"predictions\": p, \"metrics\": m})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#results[0][\"predictions\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "colors = itertools.cycle(('r', 'y', 'b', 'g'))\n",
    "\n",
    "for i, ts in enumerate(ts_data):\n",
    "    plt.plot(ts[\"future\"][\"t\"], ts[\"future\"][\"y_scaled\"], lw=3, color='g', label=\"True\")\n",
    "    for j, r in enumerate(results):\n",
    "        df = r[\"predictions\"][i] \n",
    "        c = next(colors)\n",
    "        plt.plot(ts[\"future\"][\"t\"], df[\"y\"], label=\"Model %d\" % j, color=c)\n",
    "        #lt.fill_between(ts_data[i][\"future\"][\"t\"], df[\"y_scaled_pred_lower\"], df[\"y_scaled_pred_upper\"], color=c, alpha=.05)\n",
    "    plt.legend(loc=2, prop={'size': 24})\n",
    "    plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_composition(df, y_true): \n",
    "    df.index = df[\"ds\"]\n",
    "    decomposition = seasonal_decompose(y_true - df[\"y\"])\n",
    "    plt.subplot(411) \n",
    "    plt.plot(decomposition.trend)\n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.seasonal)\n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.resid)\n",
    "    plt.subplot(414)\n",
    "    plt.plot(df[\"ds\"], y_true - pred_df[\"y\"])\n",
    "    plt.show()\n",
    "        \n",
    "    \n",
    "for i, ts in enumerate(ts_data):\n",
    "    for j, r in enumerate(results):\n",
    "        df = r[\"predictions\"][i]\n",
    "        plot_composition(df, ts[\"future\"][\"y_scaled\"].as_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Lessions Learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
