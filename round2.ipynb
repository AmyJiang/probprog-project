{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Web Traffic Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from edward.models import Normal, Laplace, Empirical\n",
    "\n",
    "from model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (24, 12)\n",
    "matplotlib.rcParams['lines.linewidth'] = 2\n",
    "matplotlib.rcParams['xtick.labelsize'] = 18\n",
    "matplotlib.rcParams['ytick.labelsize'] = 18\n",
    "matplotlib.rcParams['xtick.color'] = 'w'\n",
    "matplotlib.rcParams['ytick.color'] = 'w'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FPATH = \"./data/nfl_teams.csv\"\n",
    "SDATE = pd.datetime(2017, 7, 10)\n",
    "\n",
    "def get_timeseries(path):\n",
    "    df = pd.read_csv(path)\n",
    "    timeseries = {}\n",
    "    print(\"Loading timeseries:\")\n",
    "    for i, row in df.iterrows():\n",
    "        ts = pd.DataFrame({\"ds\": row.index[1:], \"views\": row.values[1:]})\n",
    "        ts[\"y\"] = ts[\"views\"].astype(float)\n",
    "        timeseries[row.Page] = ts\n",
    "        print(row.Page)\n",
    "        plt.plot(ts[\"ds\"], np.log(ts[\"y\"]))\n",
    "        plt.xtick(rotation=90)\n",
    "        plt.show()\n",
    "    return timeseries\n",
    "\n",
    "timeseries = get_timeseries(FPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPATH = \"./data/Valentine's_Day_en.wikipedia.org_all-access_spider.csv\"\n",
    "# df = pd.read_csv(FPATH)\n",
    "\n",
    "# Parameters\n",
    "SDATE = pd.datetime(2017, 7, 10)\n",
    "\n",
    "# Prepare dataframe\n",
    "df = timeseries[\"Atlanta_Falcons_en.wikipedia.org_mobile-web_all-agents\"]\n",
    "print(df.head())\n",
    "df[\"y\"] = np.log(df[\"y\"])\n",
    "df  = setup_dataframe(df)\n",
    "\n",
    "# Split data into train and test\n",
    "history = df[df['ds'] <= SDATE].copy()\n",
    "future = df[df['ds'] > SDATE].copy()\n",
    "print(\"History: %d, Future: %d\" % (history.shape[0], future.shape[0]))\n",
    "\n",
    "plt.plot(history['ds'],history['y'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Add a scaled t (time index) and y (#views)\n",
    "t_start = history['ds'].min()\n",
    "t_scale = history['ds'].max() - t_start\n",
    "if t_scale == 0:\n",
    "    raise ValueError(\"Timeseries start == end\")\n",
    "y_scale = history['y'].max()\n",
    "if y_scale == 0:\n",
    "    y_scale = 1\n",
    "history['t'] = (history['ds'] - t_start) / t_scale\n",
    "history['y_scaled'] = history['y'] / y_scale\n",
    "print(\"History dataframe: %d\\n\" % history.shape[0], history.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "holiday_en_us = ['2015-01-01', '2015-01-19', '2015-05-25', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', \n",
    "                 '2016-01-01', '2016-01-18', '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-26', \n",
    "                 '2017-01-01', '2017-01-02', '2017-01-16', '2017-05-29', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', \n",
    "                 '2017-12-25',\n",
    "                 '2015-02-14', '2016-02-14', '2017-02-14']\n",
    "holidays = pd.DataFrame({\n",
    "  'holiday': 'US public holiday',\n",
    "  'ds': pd.to_datetime(holiday_en_us),\n",
    "  'lower_window': -1,\n",
    "  'upper_window': 0,\n",
    "  'prior_scale': 10.0\n",
    "})\n",
    "holidays = None\n",
    "\n",
    "seasonal_features, prior_scales = make_seasonality_features(history, \n",
    "                                                            yearly=True, weekly=True, \n",
    "                                                            holidays=holidays)\n",
    "print(\"Seasonal features:\\n\")\n",
    "print(seasonal_features.columns)\n",
    "    \n",
    "K = seasonal_features.shape[1] # number of seasonal factors\n",
    "changepoints_t = get_changepoints(history, n_changepoints=25)\n",
    "S = len(changepoints_t) # number of change points\n",
    "changepoint_prior_scale = 1.0\n",
    "\n",
    "print(\"Seasonal_features: %d\\n\" % K)\n",
    "\n",
    "if holidays is not None:\n",
    "    print(\"Holidays:\\n\")\n",
    "    holiday_ds = []\n",
    "    for feature in seasonal_features:\n",
    "        if feature.split(\"_delim_\")[0] in set(holidays['holiday']):\n",
    "            holiday_ds.extend(seasonal_features[seasonal_features[feature]==1.0].index)\n",
    "    print(history.iloc[np.unique(holiday_ds)][\"ds\"])\n",
    "\n",
    "\n",
    "print(\"Changepoints: %d\" % S)\n",
    "X_train = {\n",
    "    't': history['t'].as_matrix(), # day\n",
    "    'A': get_changepoint_matrix(history, changepoints_t), # split indicator\n",
    "    'X': seasonal_features, # seasonal vectors\n",
    "    'sigmas': prior_scales, # scale on seasonality prior\n",
    "}\n",
    "\n",
    "Y_train = history['y_scaled'].as_matrix()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "t = tf.placeholder(tf.float32, shape=None, name=\"t\")        # time index\n",
    "A = tf.placeholder(tf.float32, shape=(None, S), name=\"A\")      # changepoint indicators\n",
    "t_change = tf.placeholder(tf.float32, shape=(S), name=\"t_change\") # changepoints_t\n",
    "X = tf.placeholder(tf.float32, shape=(None, K), name=\"X\")      # season vectors\n",
    "sigmas = tf.placeholder(tf.float32, shape=(K,), name=\"sigmas\")  # scale on seasonality prior\n",
    "#tau = tf.placeholder(tf.float32, shape=(), name=\"tau\")      # scale on changepoints prior\n",
    "tau = Normal(loc=tf.ones(1) * 0.05, scale=1.*tf.ones(1))\n",
    "\n",
    "k = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial slope\n",
    "m = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial intercept\n",
    "sigma_obs = Normal(loc=tf.zeros(1), scale=0.5*tf.ones(1))   # noise\n",
    "\n",
    "delta = Laplace(loc=tf.zeros(S), scale=tau*tf.ones(S))      # changepoint rate adjustment\n",
    "gamma = tf.multiply(-t_change, delta, name=\"gamma\")\n",
    "\n",
    "beta = Normal(loc=tf.zeros(K), scale=sigmas*tf.ones(K))     # seasonal\n",
    "\n",
    "trend_loc = (k + ed.dot(A, delta)) * t + (m + ed.dot(A, gamma))\n",
    "seas_loc = ed.dot(X, beta)\n",
    "y = Normal(loc = trend_loc + seas_loc, scale = sigma_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "ITR = 5000                       # Number of samples.\n",
    "\n",
    "# Init k, m\n",
    "def init_km(df):\n",
    "    i0, i1 = df['ds'].idxmin(), df['ds'].idxmax()\n",
    "    T = df['t'].iloc[i1] - df['t'].iloc[i0]\n",
    "    k = (df['y_scaled'].iloc[i1] - df['y_scaled'].iloc[i0]) / T\n",
    "    m = df['y_scaled'].iloc[i0] -  k * df['t'].iloc[i0]\n",
    "    return (k, m)\n",
    "\n",
    "kinit, minit = init_km(history)\n",
    "print(\"Initial slope / intercept: %f, %f\" % (kinit, minit))\n",
    "qk = Empirical(params=tf.Variable(kinit * tf.ones([ITR, 1])))\n",
    "qm = Empirical(params=tf.Variable(minit * tf.ones([ITR, 1])))\n",
    "qsigma_obs = Empirical(params=tf.Variable(tf.ones([ITR, 1])))\n",
    "qbeta = Empirical(params=tf.Variable(tf.zeros([ITR, K])))\n",
    "qdelta = Empirical(params=tf.Variable(tf.zeros([ITR, S])))\n",
    "qtau = Empirical(params=tf.Variable(0.05 * tf.ones([ITR, 1])))\n",
    "\n",
    "inference = ed.HMC({k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta,\n",
    "                    tau: qtau}, \n",
    "                   data={y: Y_train, \n",
    "                         t: X_train['t'],\n",
    "                         A: X_train['A'], \n",
    "                         X: X_train['X'].as_matrix(), \n",
    "                         sigmas: X_train['sigmas'], \n",
    "                         t_change: changepoints_t})\n",
    "                        #tau: changepoint_prior_scale}))\n",
    "                         \n",
    "inference.run(step_size=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scaled t and y\n",
    "future['t'] = (future['ds'] - t_start) / t_scale\n",
    "future['y_scaled'] = future['y'] / y_scale\n",
    "print(\"Future dataframe: %d\\n\" % future.shape[0], future.head())\n",
    "print()\n",
    "\n",
    "# Extract seasonality features\n",
    "future_seasonal, future_prior_scales = make_seasonality_features(future, \n",
    "                                                                 yearly=True, weekly=True,\n",
    "                                                                 holidays=holidays)\n",
    "assert(future_seasonal.shape[1] == K)\n",
    "assert(all(future_seasonal.columns == seasonal_features.columns))\n",
    "\n",
    "X_test = {\n",
    "    't': future['t'].as_matrix(), # day\n",
    "    'A': get_changepoint_matrix(future, changepoints_t), # split indicator\n",
    "    'X': future_seasonal, # seasonal vectors\n",
    "    'sigmas': future_prior_scales, # scale on seasonality prior\n",
    "}\n",
    "\n",
    "Y_test = future['y_scaled'].as_matrix()\n",
    "\n",
    "test_data = {y_post: Y_test, t: X_test['t'],\n",
    "             A: X_test['A'], X: X_test['X'].as_matrix(), \n",
    "             sigmas: X_test['sigmas'], t_change: changepoints_t}\n",
    "                        #tau: changepoint_prior_scale}))\n",
    "\n",
    "posteriors = {k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta, tau:qtau}\n",
    "\n",
    "# Evaluate test data\n",
    "y_post = ed.copy(y, posteriors)\n",
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error',  #mean_absolute_percentage_error\n",
    "                  data=test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "sess = ed.get_session()\n",
    "\n",
    "y_pred = np.array([sess.run([y_post], \n",
    "                  feed_dict=test_data) for _ in range(500)]).mean(axis=0)[0]\n",
    "\n",
    "# Metrics \n",
    "def evaluate(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    smape = np.mean(np.abs((y_true - y_pred)) / (np.abs((y_true + y_pred)))) * 100\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    print(\"MAPE = %f\" % mape)\n",
    "    print(\"SMAPE = %f\" % smape)\n",
    "    print(\"MSE = %f\" % mse)\n",
    "    \n",
    "evaluate(future['y_scaled'], y_pred)\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], y_pred)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "def median_model(history, p=-50):\n",
    "    visits = np.nan_to_num(np.nanmedian(history['y_scaled'].values[-p:]))\n",
    "    return np.ones(future.shape[0]) * visits\n",
    "\n",
    "y_pred_median = median_model(history)\n",
    "evaluate(future['y_scaled'], y_pred_median)\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], y_pred_median)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training error \n",
    "y_train_pred = np.array([sess.run([y_post], \n",
    "                                  feed_dict={t: X_train['t'],\n",
    "                                             A: X_train['A'], X: X_train['X'].as_matrix(), \n",
    "                                             sigmas: X_train['sigmas'], t_change: changepoints_t}\n",
    "                                                #tau: changepoint_prior_scale}))\n",
    "                                 ) for _ in range(500)]).mean(axis=0)[0]\n",
    "\n",
    "# Metrics \n",
    "def evalute(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    smape = np.mean(np.abs((y_true - y_pred)) / (np.abs((y_true + y_pred)))) * 100\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    print(\"MAPE = %f\" % mape)\n",
    "    print(\"SMAPE = %f\" % smape)\n",
    "    print(\"MSE = %f\" % mse)\n",
    "    \n",
    "evalute(history['y_scaled'], y_train_pred)\n",
    "plt.plot(history['ds'], history['y_scaled'])\n",
    "plt.plot(history['ds'], y_train_pred)\n",
    "plt.plot(future['ds'], y_pred)\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior check\n",
    "kmean, kstddev = sess.run([qk.mean(), qk.stddev()])\n",
    "print(\"Inferred posterior k: mean = %f, stddev = %f\" % (kmean, kstddev))\n",
    "mmean, mstddev = sess.run([qm.mean(), qm.stddev()])\n",
    "print(\"Inferred posterior m: mean = %f, stddev = %f\" % (mmean, mstddev))\n",
    "tau_mean, tau_stddev = sess.run([qtau.mean(), qtau.stddev()])\n",
    "print(\"Inferred posterior tau: mean = %f, stddev = %f\" % (tau_mean, tau_stddev))\n",
    "\n",
    "\n",
    "noise_mean, noise_stddev = sess.run([qsigma_obs.mean(), qsigma_obs.stddev()])\n",
    "print(\"Inferred posterior noise: mean = %f, stddev = %f\" % (noise_mean, noise_stddev))\n",
    "\n",
    "nburn = 500\n",
    "stride = 10\n",
    "sns.distplot(qk.params.eval()[nburn:ITR:stride])\n",
    "plt.show()\n",
    "sns.distplot(qm.params.eval()[nburn:ITR:stride])\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(qtau.params.eval()[nburn:ITR:stride])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "sess = ed.get_session()\n",
    "\n",
    "# TODO: mean?\n",
    "y_pred = sess.run([y_post.mean()], \n",
    "                  feed_dict={t: X_test['t'],\n",
    "                             A: X_test['A'], \n",
    "                             X: X_test['X'].as_matrix(), \n",
    "                             sigmas: X_test['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale})[0]\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Trend = k + ed.dot(A, delta)) * t + (m + ed.dot(A, gamma)\n",
    "trend_post = ed.copy(trend_loc, {k: qk, m: qm, delta:qdelta})\n",
    "seas_post = ed.copy(seas_loc, {beta: qbeta})\n",
    "trend_pred, seas_pred = sess.run([trend_post, seas_post], \n",
    "                                 feed_dict={t: X_test['t'],\n",
    "                             A: X_test['A'], \n",
    "                             X: X_test['X'].as_matrix(), \n",
    "                             sigmas: X_test['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale})\n",
    "\n",
    "# Plot trend\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], trend_pred)\n",
    "plt.show()\n",
    "\n",
    "# Plot seasonal\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], seas_pred)\n",
    "plt.show()\n",
    "\n",
    "# Plot trend + seasonal (no noise)\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], trend_pred + seas_pred) # no noise\n",
    "plt.show()\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 200 * np.mean(diff)\n",
    "\n",
    "# Metrics \n",
    "mape = np.mean(np.abs((future['y_scaled'] - y_pred) / future['y_scaled'])) * 100\n",
    "mse = ((future['y_scaled'] - y_pred) ** 2).mean()\n",
    "#mse = tf.reduce_mean(tf.square(y_pred - future['y_scaled']))\n",
    "print(\"MAPE = %f\" % mape)\n",
    "print(\"SMAPE = %f\" % mape)\n",
    "print(\"MSE = %f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from fbprophet import Prophet\n",
    "\n",
    "# # Facebook Prophet\n",
    "# df = pd.read_csv(\"./data/Selena_en.wikipedia.org_all-access_spider.csv\")\n",
    "# df[\"y\"] = np.log(df[\"y\"])\n",
    "# df.head()\n",
    "# m_pp = Prophet()\n",
    "# m_pp.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# future = m_pp.make_future_dataframe(periods=60)\n",
    "# future.tail()\n",
    "# forecast = m_pp.predict(future)\n",
    "# forecast[['ds', 'trend', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "# m_pp.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# m_pp.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
