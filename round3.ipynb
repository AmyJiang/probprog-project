{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Web Traffic Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from edward.models import Normal, Laplace, Empirical\n",
    "\n",
    "from model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (24, 12)\n",
    "matplotlib.rcParams['lines.linewidth'] = 2\n",
    "matplotlib.rcParams['xtick.labelsize'] = 18\n",
    "matplotlib.rcParams['ytick.labelsize'] = 18\n",
    "matplotlib.rcParams['xtick.color'] = 'w'\n",
    "matplotlib.rcParams['ytick.color'] = 'w'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FPATH = \"./data/nfl_teams.csv\"\n",
    "SDATE = pd.datetime(2017, 7, 10)\n",
    "\n",
    "def get_timeseries(path):\n",
    "    df = pd.read_csv(path)\n",
    "    timeseries = {}\n",
    "    print(\"Loading timeseries:\")\n",
    "    for i, row in df.iterrows():\n",
    "        ts = pd.DataFrame({\"ds\": row.index[1:], \"views\": row.values[1:]})\n",
    "        timeseries[row.Page] = ts\n",
    "        print(row.Page)\n",
    "        #plt.plot(ts[\"ds\"], np.log(ts[\"y\"]))\n",
    "        #plt.xticks(rotation=90)\n",
    "        #plt.show()\n",
    "    return timeseries\n",
    "\n",
    "timeseries = get_timeseries(FPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def split_train_test(df, sdate=pd.datetime(2017, 7, 10)):\n",
    "    # Prepare dataframe\n",
    "    df[\"y\"] = df[\"views\"].astype(float)\n",
    "    df[\"y\"] = np.log(df[\"y\"])\n",
    "    df = setup_dataframe(df)\n",
    "    \n",
    "    # Split data into train and test\n",
    "    history = df[df['ds'] <= SDATE].copy()\n",
    "    future = df[df['ds'] > SDATE].copy()\n",
    "    print(\"[+] History: %d, Future: %d\" % (history.shape[0], future.shape[0]))\n",
    "    \n",
    "    # Add a scaled t (time index) and y (#views)\n",
    "    t_start = history['ds'].min()\n",
    "    t_scale = history['ds'].max() - t_start\n",
    "    if t_scale == 0:\n",
    "        raise ValueError(\"Timeseries start == end\")\n",
    "    y_scale = history['y'].max()\n",
    "    if y_scale == 0:\n",
    "        y_scale = 1\n",
    "    history['t'] = (history['ds'] - t_start) / t_scale\n",
    "    history['y_scaled'] = history['y'] / y_scale\n",
    "    future['t'] = (future['ds'] - t_start) / t_scale\n",
    "    future['y_scaled'] = future['y'] / y_scale\n",
    "    \n",
    "    plt.plot(history['ds'],history['y'])\n",
    "    plt.plot(future['ds'],future['y'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    return (history, future, y_scale)\n",
    "    \n",
    "\n",
    "SDATE = pd.datetime(2017, 7, 10)\n",
    "pages = [\"Atlanta_Falcons_en.wikipedia.org_mobile-web_all-agents\", \"Dallas_Cowboys_en.wikipedia.org_mobile-web_all-agents\"]\n",
    "ts_data = []\n",
    "#pages = [\"\"]\n",
    "#pages = list(timeseries.keys())[:-1]\n",
    "#pages = pages[:-1]\n",
    "for p in pages:\n",
    "    print(\"Preparing timeseries \", p)\n",
    "    df = timeseries[p]\n",
    "    history, future, y_scale = split_train_test(df, SDATE)\n",
    "    ts_data.append({\n",
    "        \"history\": history, \"future\": future, \"y_scale\": y_scale\n",
    "    })\n",
    "    print(future.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "holiday_en_us = ['2015-01-01', '2015-01-19', '2015-05-25', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', \n",
    "                 '2016-01-01', '2016-01-18', '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-26', \n",
    "                 '2017-01-01', '2017-01-02', '2017-01-16', '2017-05-29', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', \n",
    "                 '2017-12-25',\n",
    "                 '2015-02-14', '2016-02-14', '2017-02-14']\n",
    "holidays = pd.DataFrame({\n",
    "  'holiday': 'US public holiday',\n",
    "  'ds': pd.to_datetime(holiday_en_us),\n",
    "  'lower_window': -1,\n",
    "  'upper_window': 0,\n",
    "  'prior_scale': 10.0\n",
    "})\n",
    "holidays = None\n",
    "\n",
    "def extract_features(df):\n",
    "    seasonal_features, prior_scales = make_seasonality_features(df, \n",
    "                                                            yearly=True, weekly=True, \n",
    "                                                            holidays=holidays)\n",
    "    K = seasonal_features.shape[1] # number of seasonal factors\n",
    "    print(\"[+] %d Seasonal features\" % K) #, list(seasonal_features.columns))\n",
    "    if holidays is not None:\n",
    "        holiday_ds = {}\n",
    "        for feature in seasonal_features:\n",
    "            if feature.split(\"_delim_\")[0] in set(holidays['holiday']):\n",
    "                holiday_ds[feature] = seasonal_features[seasonal_features[feature]==1.0].shape[0]\n",
    "        print(\"%d Holidays\" % len(holiday_ds), holiday_ds) \n",
    "\n",
    "    changepoints_t = get_changepoints(df, n_changepoints=25)\n",
    "    S = len(changepoints_t) # number of change points\n",
    "    print(\"[+] %d changepoints\" % S)\n",
    "\n",
    "    return  {\n",
    "        't': df['t'].as_matrix(), # time index\n",
    "        'A': get_changepoint_matrix(df, changepoints_t), # split indicator\n",
    "        'X': seasonal_features, # seasonal vectors\n",
    "        'sigmas': prior_scales, # scale on seasonality prior\n",
    "        't_change': changepoints_t\n",
    "    }\n",
    "\n",
    "\n",
    "ts = ts_data[0]\n",
    "print(\"Extracting features\")\n",
    "train_data = extract_features(ts[\"history\"])\n",
    "test_data = extract_features(ts[\"future\"])\n",
    "assert(all(train_data[\"X\"].columns ==  test_data[\"X\"].columns))\n",
    "assert(len(train_data[\"t_change\"]) ==  len(test_data[\"t_change\"]))\n",
    "print(train_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(name, N_TS, S, K):\n",
    "    with tf.name_scope(name):\n",
    "        t = tf.placeholder(tf.float32, shape=None, name=\"t\")              # time index\n",
    "        A = tf.placeholder(tf.float32, shape=(None, S), name=\"A\")         # changepoint indicators\n",
    "        t_change = tf.placeholder(tf.float32, shape=(S), name=\"t_change\") # changepoints_t\n",
    "        X = tf.placeholder(tf.float32, shape=(None, K), name=\"X\")         # season vectors\n",
    "        sigmas = tf.placeholder(tf.float32, shape=(K,), name=\"sigmas\")    # scale on seasonality prior\n",
    "        return {\n",
    "            \"t\": t, \"A\": A, \"t_change\": t_change, \"X\": X, \"sigmas\": sigmas \n",
    "        }\n",
    "                                \n",
    "def get_posts(params, posts, i=None):\n",
    "    if i == None:\n",
    "        return {\n",
    "            params[i][k]:v for i, ps in posts.items() for k, v in ps.items()\n",
    "        }\n",
    "    else:\n",
    "        p = {params[i][k]:v for k, v in posts[i].items()}\n",
    "        if -1 in posts: # common parameters\n",
    "            p.update({ params[-1][k]:v for k, v in posts[-1].items() })\n",
    "        return p\n",
    "    \n",
    "def run_inference(name, data, params, posts, ITR=5000):\n",
    "    # train_data and ts_data are global\n",
    "    with tf.name_scope(name):\n",
    "        data_dict = {data[k]:v for k, v in train_data.items()}\n",
    "        # add y true\n",
    "        for i in range(len(ts_data)):\n",
    "            data_dict.update({\n",
    "                params[i][\"y\"]: ts_data[i][\"history\"][\"y_scaled\"].as_matrix()\n",
    "            })\n",
    "        posts_dict = get_posts(params, posts)\n",
    "        inference = ed.HMC(posts_dict, data=data_dict)\n",
    "        inference.run(step_size=5e-4)\n",
    "    \n",
    "# Prediction\n",
    "def evaluate(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    smape = np.mean(np.abs((y_true - y_pred)) / (np.abs((y_true + y_pred)))) * 200\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    print(\"MAPE = %f\" % mape)\n",
    "    print(\"SMAPE = %f\" % smape)\n",
    "    print(\"MSE = %f\" % mse)\n",
    "    \n",
    "def predict(name, data, params, posts):\n",
    "    # test_data and ts_data are global\n",
    "    with tf.name_scope(name):\n",
    "        data_dict = {data[k]:v for k, v in test_data.items()}\n",
    "        sess = ed.get_session()\n",
    "        for i, ts in enumerate(ts_data):\n",
    "            posts_dict = get_posts(params, posts, i=i)\n",
    "            y_post = ed.copy(params[i][\"y\"], posts_dict) \n",
    "            y_pred = np.array([sess.run([y_post], \n",
    "                                    feed_dict=data_dict) for _ in range(500)]).mean(axis=0)[0]\n",
    "            y_true = ts[\"future\"][\"y_scaled\"]\n",
    "            evaluate(y_true, y_pred)\n",
    "            plt.plot(ts[\"future\"][\"ds\"], y_true)\n",
    "            plt.plot(ts[\"future\"][\"ds\"], y_pred)\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.show()\n",
    "        \n",
    "#kmean, kstddev = sess.run([model1.posts[i][\"k\"].mean(), model1.posts[i][\"k\"].stddev()])\n",
    "      #      print(\"Inferred posterior k: mean = %f, stddev = %f\" % (kmean, kstddev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Model1(object):\n",
    "    def __init__(self, N_TS, S, K):\n",
    "        self.N_TS = N_TS\n",
    "        self.S = S\n",
    "        self.K = K\n",
    "        self.name = \"model1\"\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.name_scope(self.name):\n",
    "            self.data = build_data(self.name, self.N_TS, self.S, self.K)\n",
    "            self.params = {}      \n",
    "            for i in range(self.N_TS):\n",
    "                k = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))     # initial slope\n",
    "                m = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))     # initial intercept\n",
    "                sigma_obs = Normal(loc=tf.zeros(1), scale=0.5*tf.ones(1))   # noise\n",
    "                tau = Normal(loc=tf.ones(1) * 0.05, scale=1.*tf.ones(1))    # changepoint prior scale\n",
    "                delta = Laplace(loc=tf.zeros(self.S), scale=tau*tf.ones(self.S))    # changepoint rate adjustment\n",
    "                gamma = tf.multiply(-self.data[\"t_change\"], delta)\n",
    "                beta = Normal(loc=tf.zeros(self.K), \n",
    "                              scale=self.data[\"sigmas\"]*tf.ones(self.K))      # seasonal\n",
    "                trend_loc = (k + ed.dot(self.data[\"A\"], delta)) * self.data[\"t\"] + \\\n",
    "                            (m + ed.dot(self.data[\"A\"], gamma))\n",
    "                seas_loc = ed.dot(self.data[\"X\"], beta)\n",
    "                y = Normal(loc = trend_loc + seas_loc, scale = sigma_obs)\n",
    "                self.params[i] = {\n",
    "                    \"k\": k, \"m\": m,\n",
    "                    \"sigma_obs\": sigma_obs,\n",
    "                    \"tau\": tau, \"delta\": delta,\n",
    "                    \"beta\": beta,\n",
    "                    \"trend_loc\": trend_loc, \"seas_loc\": seas_loc,\n",
    "                    \"y\": y\n",
    "                }\n",
    "                             \n",
    "    def build_posts(self, ITR, ts_data):\n",
    "        assert(self.N_TS == len(ts_data))\n",
    "        self.ITR = ITR\n",
    "        self.posts = {}\n",
    "        with tf.name_scope(self.name):\n",
    "            for i in range(self.N_TS): \n",
    "                kinit, minit = init_km(ts_data[i][\"history\"])\n",
    "                print(\"[+] Initial slope / intercept: %f, %f\" % (kinit, minit))\n",
    "                qbeta = Empirical(params=tf.Variable(tf.zeros([ITR, self.K])))\n",
    "                qk = Empirical(params=tf.Variable(kinit * tf.ones([ITR, 1])))\n",
    "                qm = Empirical(params=tf.Variable(minit * tf.ones([ITR, 1])))\n",
    "                qsigma_obs = Empirical(params=tf.Variable(tf.ones([ITR, 1])))\n",
    "                qdelta = Empirical(params=tf.Variable(tf.zeros([ITR, self.S])))\n",
    "                qtau = Empirical(params=tf.Variable(0.05 * tf.ones([ITR, 1])))\n",
    "                self.posts[i] = {\n",
    "                    \"k\": qk, \"m\": qm,\n",
    "                    \"sigma_obs\": qsigma_obs, \n",
    "                    \"delta\": qdelta,\n",
    "                    \"tau\": qtau,\n",
    "                    \"beta\": qbeta\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model2(object):\n",
    "    def __init__(self, N_TS, S, K):\n",
    "        self.N_TS = N_TS\n",
    "        self.S = S\n",
    "        self.K = K\n",
    "        self.name = \"model2\"\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.name_scope(self.name):\n",
    "            self.data = build_data(self.name, self.N_TS, self.S, self.K)\n",
    "            self.params = {}      \n",
    "             \n",
    "            # Common prior\n",
    "            beta = Normal(loc=tf.zeros(self.K), scale=self.data[\"sigmas\"]*tf.ones(self.K)) \n",
    "            self.params[-1] = {\"beta\": beta}\n",
    "            \n",
    "            for i in range(self.N_TS):\n",
    "                k = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))     # initial slope\n",
    "                m = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))     # initial intercept\n",
    "                sigma_obs = Normal(loc=tf.zeros(1), scale=0.5*tf.ones(1))   # noise\n",
    "                tau = Normal(loc=tf.ones(1) * 0.05, scale=1.*tf.ones(1))    # changepoint prior scale\n",
    "                delta = Laplace(loc=tf.zeros(self.S), scale=tau*tf.ones(self.S))    # changepoint rate adjustment\n",
    "                gamma = tf.multiply(-self.data[\"t_change\"], delta)\n",
    "\n",
    "                trend_loc = (k + ed.dot(self.data[\"A\"], delta)) * self.data[\"t\"] + \\\n",
    "                            (m + ed.dot(self.data[\"A\"], gamma))\n",
    "                seas_loc = ed.dot(self.data[\"X\"], beta)\n",
    "                y = Normal(loc = trend_loc + seas_loc, scale = sigma_obs)\n",
    "                self.params[i] = {\n",
    "                    \"k\": k, \"m\": m,  \"sigma_obs\": sigma_obs,\n",
    "                    \"tau\": tau, \"delta\": delta,\n",
    "                    \"trend_loc\": trend_loc, \"seas_loc\": seas_loc, \"y\": y\n",
    "                }\n",
    "                             \n",
    "    def build_posts(self, ITR, ts_data):\n",
    "        assert(self.N_TS == len(ts_data))\n",
    "        self.ITR = ITR\n",
    "        self.posts = {}\n",
    "        with tf.name_scope(self.name):\n",
    "            qbeta = Empirical(params=tf.Variable(tf.zeros([ITR, self.K])))\n",
    "            self.posts[-1] = {\"beta\": qbeta }\n",
    "            for i in range(self.N_TS): \n",
    "                kinit, minit = init_km(ts_data[i][\"history\"])\n",
    "                print(\"[+] Initial slope / intercept: %f, %f\" % (kinit, minit))\n",
    "                qk = Empirical(params=tf.Variable(kinit * tf.ones([ITR, 1])))\n",
    "                qm = Empirical(params=tf.Variable(minit * tf.ones([ITR, 1])))\n",
    "                qsigma_obs = Empirical(params=tf.Variable(tf.ones([ITR, 1])))\n",
    "                qdelta = Empirical(params=tf.Variable(tf.zeros([ITR, self.S])))\n",
    "                qtau = Empirical(params=tf.Variable(0.05 * tf.ones([ITR, 1])))\n",
    "                self.posts[i] = {\n",
    "                    \"k\": qk, \"m\": qm,\n",
    "                    \"sigma_obs\": qsigma_obs, \n",
    "                    \"delta\": qdelta,\n",
    "                    \"tau\": qtau,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(object):\n",
    "    def __init__(self, N_TS, S, K):\n",
    "        self.N_TS = N_TS\n",
    "        self.S = S\n",
    "        self.K = K\n",
    "        self.name = \"model2\"\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.name_scope(self.name):\n",
    "            self.data = build_data(self.name, self.N_TS, self.S, self.K)\n",
    "            self.params = {}      \n",
    "             \n",
    "            # Common prior\n",
    "            gbeta = Normal(loc=tf.zeros(self.K), scale=self.data[\"sigmas\"]*tf.ones(self.K)) \n",
    "            self.params[-1] = {\"gbeta\": gbeta}\n",
    "            \n",
    "            for i in range(self.N_TS):\n",
    "                k = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))     # initial slope\n",
    "                m = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))     # initial intercept\n",
    "                sigma_obs = Normal(loc=tf.zeros(1), scale=0.5*tf.ones(1))   # noise\n",
    "                tau = Normal(loc=tf.ones(1) * 0.05, scale=1.*tf.ones(1))    # changepoint prior scale\n",
    "                delta = Laplace(loc=tf.zeros(self.S), scale=tau*tf.ones(self.S))    # changepoint rate adjustment\n",
    "                gamma = tf.multiply(-self.data[\"t_change\"], delta)\n",
    "                trend_loc = (k + ed.dot(self.data[\"A\"], delta)) * self.data[\"t\"] + \\\n",
    "                            (m + ed.dot(self.data[\"A\"], gamma))\n",
    "                beta = Normal(loc=tf.zeros(self.K), scale=self.data[\"sigmas\"]*tf.ones(self.K)) \n",
    "                seas_loc = ed.dot(self.data[\"X\"], beta)\n",
    "                y = Normal(loc = trend_loc + seas_loc, scale = sigma_obs)\n",
    "                self.params[i] = {\n",
    "                    \"k\": k, \"m\": m,  \"sigma_obs\": sigma_obs,\n",
    "                    \"tau\": tau, \"delta\": delta, \"beta\": beta,\n",
    "                    \"trend_loc\": trend_loc, \"seas_loc\": seas_loc, \"y\": y\n",
    "                }\n",
    "                             \n",
    "    def build_posts(self, ITR, ts_data):\n",
    "        assert(self.N_TS == len(ts_data))\n",
    "        self.ITR = ITR\n",
    "        self.posts = {}\n",
    "        with tf.name_scope(self.name):\n",
    "            qgbeta = Empirical(params=tf.Variable(tf.zeros([ITR, self.K])))\n",
    "            self.posts[-1] = {\"gbeta\": qgbeta }\n",
    "            for i in range(self.N_TS): \n",
    "                kinit, minit = init_km(ts_data[i][\"history\"])\n",
    "                print(\"[+] Initial slope / intercept: %f, %f\" % (kinit, minit))\n",
    "                qk = Empirical(params=tf.Variable(kinit * tf.ones([ITR, 1])))\n",
    "                qm = Empirical(params=tf.Variable(minit * tf.ones([ITR, 1])))\n",
    "                qsigma_obs = Empirical(params=tf.Variable(tf.ones([ITR, 1])))\n",
    "                qdelta = Empirical(params=tf.Variable(tf.zeros([ITR, self.S])))\n",
    "                qtau = Empirical(params=tf.Variable(0.05 * tf.ones([ITR, 1])))\n",
    "                qbeta = Empirical(params=tf.Variable(tf.zeros([ITR, self.K])))\n",
    "                self.posts[i] = {\n",
    "                    \"k\": qk, \"m\": qm,\n",
    "                    \"sigma_obs\": qsigma_obs, \n",
    "                    \"delta\": qdelta, \"beta\": qbeta,\n",
    "                    \"tau\": qtau,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model1(len(ts_data),\n",
    "                len(train_data[\"t_change\"]),         # number of change points\n",
    "                train_data[\"X\"].shape[1])            # number of seasonal factors\n",
    "model1.build_model()\n",
    "\n",
    "ITR = 5000                       # Number of samples.\n",
    "model1.build_posts(ITR, ts_data)\n",
    "run_inference(model1.name, model1.data, model1.params, model1.posts, ITR)\n",
    "predict(model1.name, model1.data, model1.params, model1.posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model2(len(ts_data),\n",
    "                len(train_data[\"t_change\"]),         # number of change points\n",
    "                train_data[\"X\"].shape[1])            # number of seasonal factors\n",
    "model1.build_model()\n",
    "\n",
    "ITR = 5000                       # Number of samples.\n",
    "model1.build_posts(ITR, ts_data)\n",
    "run_inference(model1.name, model1.data, model1.params, model1.posts, ITR)\n",
    "predict(model1.name, model1.data, model1.params, model1.posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model3(len(ts_data),\n",
    "                len(train_data[\"t_change\"]),         # number of change points\n",
    "                train_data[\"X\"].shape[1])            # number of seasonal factors\n",
    "model1.build_model()\n",
    "\n",
    "ITR = 5000                       # Number of samples.\n",
    "model1.build_posts(ITR, ts_data)\n",
    "run_inference(model1.name, model1.data, model1.params, model1.posts, ITR)\n",
    "predict(model1.name, model1.data, model1.params, model1.posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "def median_model(train, size, p=-50):\n",
    "    visits = np.nan_to_num(np.nanmedian(train[-p:]))\n",
    "    return np.ones(size) * visits\n",
    "\n",
    "for i, ts in enumerate(ts_data):\n",
    "    print(\"Median model for %d\" % i)\n",
    "    y_true = ts[\"future\"][\"y_scaled\"]\n",
    "    y_pred_median = median_model(ts[\"history\"][\"y_scaled\"], len(y_true))\n",
    "    evaluate(y_true, y_pred_median)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training error \n",
    "# y_train_pred = np.array([sess.run([y_post], \n",
    "#                                   feed_dict={t: X_train['t'],\n",
    "#                                              A: X_train['A'], X: X_train['X'].as_matrix(), \n",
    "#                                              sigmas: X_train['sigmas'], t_change: changepoints_t}\n",
    "#                                                 #tau: changepoint_prior_scale}))\n",
    "#                                  ) for _ in range(500)]).mean(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Posterior check\n",
    "kmean, kstddev = sess.run([qk.mean(), qk.stddev()])\n",
    "print(\"Inferred posterior k: mean = %f, stddev = %f\" % (kmean, kstddev))\n",
    "mmean, mstddev = sess.run([qm.mean(), qm.stddev()])\n",
    "print(\"Inferred posterior m: mean = %f, stddev = %f\" % (mmean, mstddev))\n",
    "tau_mean, tau_stddev = sess.run([qtau.mean(), qtau.stddev()])\n",
    "print(\"Inferred posterior tau: mean = %f, stddev = %f\" % (tau_mean, tau_stddev))\n",
    "\n",
    "\n",
    "noise_mean, noise_stddev = sess.run([qsigma_obs.mean(), qsigma_obs.stddev()])\n",
    "print(\"Inferred posterior noise: mean = %f, stddev = %f\" % (noise_mean, noise_stddev))\n",
    "\n",
    "nburn = 500\n",
    "stride = 10\n",
    "sns.distplot(qk.params.eval()[nburn:ITR:stride])\n",
    "plt.show()\n",
    "sns.distplot(qm.params.eval()[nburn:ITR:stride])\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(qtau.params.eval()[nburn:ITR:stride])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
