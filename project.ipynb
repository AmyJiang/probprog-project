{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Web Traffic Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from edward.models import Normal, Laplace, Empirical\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (24, 12)\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['xtick.labelsize'] = 18\n",
    "mpl.rcParams['ytick.labelsize'] = 18\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "FPATH = \"./data/Valentine's_Day_en.wikipedia.org_all-access_spider.csv\"\n",
    "SDATE = pd.datetime(2017, 7, 10)\n",
    "\n",
    "# Prepare dataframe\n",
    "df = pd.read_csv(FPATH)\n",
    "df[\"views\"] = df[\"y\"]\n",
    "df[\"y\"] = np.log(df[\"y\"])\n",
    "df  = setup_dataframe(df)\n",
    "\n",
    "# Split data into train and test\n",
    "history = df[df['ds'] <= SDATE].copy()\n",
    "future = df[df['ds'] > SDATE].copy()\n",
    "print(\"History: %d, Future: %d\" % (history.shape[0], future.shape[0]))\n",
    "\n",
    "plt.plot(history['ds'],history['y'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Add a scaled t (time index) and y (#views)\n",
    "t_start = history['ds'].min()\n",
    "t_scale = history['ds'].max() - t_start\n",
    "if t_scale == 0:\n",
    "    raise ValueError(\"Timeseries start == end\")\n",
    "y_scale = history['y'].max()\n",
    "if y_scale == 0:\n",
    "    y_scale = 1\n",
    "history['t'] = (history['ds'] - t_start) / t_scale\n",
    "history['y_scaled'] = history['y'] / y_scale\n",
    "print(\"History dataframe: %d\\n\" % history.shape[0], history.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "holiday_en_us = ['2015-01-01', '2015-01-19', '2015-05-25', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', \n",
    "                 '2016-01-01', '2016-01-18', '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-26', \n",
    "                 '2017-01-01', '2017-01-02', '2017-01-16', '2017-05-29', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', \n",
    "                 '2017-12-25',\n",
    "                 '2015-02-14', '2016-02-14', '2017-02-14']\n",
    "holidays = pd.DataFrame({\n",
    "  'holiday': 'US public holiday',\n",
    "  'ds': pd.to_datetime(holiday_en_us),\n",
    "  'lower_window': -1,\n",
    "  'upper_window': 0,\n",
    "  'prior_scale': 50.0\n",
    "})\n",
    "# holidays = None\n",
    "\n",
    "seasonal_features, prior_scales = make_seasonality_features(history, yearly=True, weekly=True, \n",
    "                                                            holidays=holidays)\n",
    "print(\"Seasonal features:\\n\")\n",
    "print(seasonal_features.columns)\n",
    "    \n",
    "K = seasonal_features.shape[1] # number of seasonal factors\n",
    "changepoints_t = get_changepoints(history, n_changepoints=25)\n",
    "S = len(changepoints_t) # number of change points\n",
    "changepoint_prior_scale = 0.05\n",
    "\n",
    "print(\"Seasonal_features: %d\\n\" % K)\n",
    "\n",
    "if holidays is not None:\n",
    "    print(\"Holidays:\\n\")\n",
    "    holiday_ds = []\n",
    "    for feature in seasonal_features:\n",
    "        if feature.split(\"_delim_\")[0] in set(holidays['holiday']):\n",
    "            holiday_ds.extend(seasonal_features[seasonal_features[feature]==1.0].index)\n",
    "    print(history.iloc[np.unique(holiday_ds)][\"ds\"])\n",
    "\n",
    "\n",
    "print(\"Changepoints: %d\" % S)\n",
    "X_train = {\n",
    "    't': history['t'].as_matrix(), # day\n",
    "    'A': get_changepoint_matrix(history, changepoints_t), # split indicator\n",
    "    'X': seasonal_features, # seasonal vectors\n",
    "    'sigmas': prior_scales, # scale on seasonality prior\n",
    "}\n",
    "\n",
    "Y_train = history['y_scaled'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "t = tf.placeholder(tf.float32, shape=None, name=\"t\")        # time index\n",
    "A = tf.placeholder(tf.float32, shape=(None, S), name=\"A\")      # changepoint indicators\n",
    "t_change = tf.placeholder(tf.float32, shape=(S), name=\"t_change\") # changepoints_t\n",
    "X = tf.placeholder(tf.float32, shape=(None, K), name=\"X\")      # season vectors\n",
    "sigmas = tf.placeholder(tf.float32, shape=(K,), name=\"sigmas\")  # scale on seasonality prior\n",
    "tau = tf.placeholder(tf.float32, shape=(), name=\"tau\")      # scale on changepoints prior\n",
    "       \n",
    "k = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial slope\n",
    "m = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial intercept\n",
    "sigma_obs = Normal(loc=tf.zeros(1), scale=0.5*tf.ones(1))   # noise\n",
    "\n",
    "delta = Laplace(loc=tf.zeros(S), scale=tau*tf.ones(S))      # changepoint rate adjustment\n",
    "gamma = tf.multiply(-t_change, delta, name=\"gamma\")\n",
    "\n",
    "beta = Normal(loc=tf.zeros(K), scale=sigmas*tf.ones(K))     # seasonal\n",
    "\n",
    "trend_loc = (k + ed.dot(A, delta)) * t + (m + ed.dot(A, gamma))\n",
    "seas_loc = ed.dot(X, beta)\n",
    "y = Normal(loc = trend_loc + seas_loc, scale = sigma_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "ITR = 5000                       # Number of samples.\n",
    "\n",
    "# Init k, m\n",
    "def init_km(df):\n",
    "    i0, i1 = df['ds'].idxmin(), df['ds'].idxmax()\n",
    "    T = df['t'].iloc[i1] - df['t'].iloc[i0]\n",
    "    k = (df['y_scaled'].iloc[i1] - df['y_scaled'].iloc[i0]) / T\n",
    "    m = df['y_scaled'].iloc[i0] -  k * df['t'].iloc[i0]\n",
    "    return (k, m)\n",
    "\n",
    "kinit, minit = init_km(history)\n",
    "print(\"Initial slope / intercept: %f, %f\" % (kinit, minit))\n",
    "qk = Empirical(params=tf.Variable(kinit * tf.ones([ITR, 1])))\n",
    "qm = Empirical(params=tf.Variable(minit * tf.ones([ITR, 1])))\n",
    "qsigma_obs = Empirical(params=tf.Variable(tf.ones([ITR, 1])))\n",
    "qbeta = Empirical(params=tf.Variable(tf.zeros([ITR, K])))\n",
    "qdelta = Empirical(params=tf.Variable(tf.zeros([ITR, S])))\n",
    "\n",
    "inference = ed.HMC({k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta}, \n",
    "                   data={y: Y_train, \n",
    "                         t: X_train['t'],\n",
    "                         A: X_train['A'], \n",
    "                         X: X_train['X'].as_matrix(), \n",
    "                         sigmas: X_train['sigmas'], \n",
    "                         t_change: changepoints_t,\n",
    "                         tau: changepoint_prior_scale})\n",
    "inference.run(step_size=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scaled t and y\n",
    "future['t'] = (future['ds'] - t_start) / t_scale\n",
    "future['y_scaled'] = future['y'] / y_scale\n",
    "print(\"Future dataframe: %d\\n\" % future.shape[0], future.head())\n",
    "print()\n",
    "\n",
    "# Extract seasonality features\n",
    "future_seasonal, future_prior_scales = make_seasonality_features(future, \n",
    "                                                                 yearly=True, weekly=True,\n",
    "                                                                 holidays=holidays)\n",
    "assert(future_seasonal.shape[1] == K)\n",
    "assert(all(future_seasonal.columns == seasonal_features.columns))\n",
    "\n",
    "X_test = {\n",
    "    't': future['t'].as_matrix(), # day\n",
    "    'A': get_changepoint_matrix(future, changepoints_t), # split indicator\n",
    "    'X': future_seasonal, # seasonal vectors\n",
    "    'sigmas': future_prior_scales, # scale on seasonality prior\n",
    "}\n",
    "\n",
    "Y_test = future['y_scaled'].as_matrix()\n",
    "\n",
    "# Prediction\n",
    "y_post = ed.copy(y, {k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta})\n",
    "sess = ed.get_session()\n",
    "y_pred = np.array([sess.run([y_post], \n",
    "                  feed_dict={t: X_test['t'],\n",
    "                             A: X_test['A'], \n",
    "                             X: X_test['X'].as_matrix(), \n",
    "                             sigmas: X_test['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale}) for _ in range(500)]).mean(axis=0)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate test data\n",
    "# print(\"Mean squared error on test data:\")\n",
    "# print(ed.evaluate('mean_squared_error',  #mean_absolute_percentage_error\n",
    "#                   data={y_post: Y_test, \n",
    "#                         t: X_test['t'],\n",
    "#                         A: X_test['A'], \n",
    "#                         X: X_test['X'].as_matrix(), \n",
    "#                         sigmas: X_test['sigmas'], \n",
    "#                         t_change: changepoints_t,\n",
    "#                         tau: changepoint_prior_scale}))\n",
    "\n",
    "# Metrics \n",
    "def evalute(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    smape = np.mean(np.abs((y_true - y_pred)) / (np.abs((y_true + y_pred)))) * 100\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    print(\"MAPE = %f\" % mape)\n",
    "    print(\"SMAPE = %f\" % smape)\n",
    "    print(\"MSE = %f\" % mse)\n",
    "    \n",
    "def plot_evaluate(ds, y_true, y_pred):\n",
    "    plt.plot(ds, y_true)\n",
    "    plt.plot(ds, y_pred)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "print(\"Evalaute test data\") \n",
    "evalute(future['y_scaled'], y_pred)\n",
    "plot_evaluate(future['ds'],future['y_scaled'],y_pred)\n",
    "plt.show()\n",
    "\n",
    "print(\"Test data before log\") \n",
    "evalute(future['views'],np.exp(y_pred * y_scale))\n",
    "plot_evaluate(future['ds'],future['views'],np.exp(y_pred * y_scale))\n",
    "plt.show()\n",
    "# future.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training error \n",
    "y_train_pred = np.array([sess.run([y_post], \n",
    "                  feed_dict={t: X_train['t'],\n",
    "                             A: X_train['A'], \n",
    "                             X: X_train['X'].as_matrix(), \n",
    "                             sigmas: X_train['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale}) for _ in range(500)]).mean(axis=0)[0]\n",
    "\n",
    "\n",
    "print(\"Evalaute train data\") \n",
    "evalute(history['y_scaled'], y_train_pred)\n",
    "plot_evaluate(history['ds'], history['y_scaled'],y_train_pred)\n",
    "plot_evaluate(future['ds'],future['y_scaled'],y_pred)\n",
    "plt.show()\n",
    "\n",
    "print(\"train data before log\") \n",
    "evalute(history['views'],np.exp(y_train_pred * y_scale))\n",
    "plot_evaluate(history['ds'], history['views'],np.exp(y_train_pred * y_scale))\n",
    "plot_evaluate(future['ds'],future['views'],np.exp(y_pred * y_scale))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Check & PPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior check\n",
    "kmean, kstddev = sess.run([qk.mean(), qk.stddev()])\n",
    "print(\"Inferred posterior k: mean = %f, stddev = %f\" % (kmean, kstddev))\n",
    "mmean, mstddev = sess.run([qm.mean(), qm.stddev()])\n",
    "print(\"Inferred posterior m: mean = %f, stddev = %f\" % (mmean, mstddev))\n",
    "\n",
    "noise_mean, noise_stddev = sess.run([qsigma_obs.mean(), qsigma_obs.stddev()])\n",
    "print(\"Inferred posterior noise: mean = %f, stddev = %f\" % (noise_mean, noise_stddev))\n",
    "\n",
    "nburn = 500\n",
    "stride = 10\n",
    "sns.distplot(qk.params.eval()[nburn:ITR:stride])\n",
    "plt.show()\n",
    "sns.distplot(qm.params.eval()[nburn:ITR:stride])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.__version__\n",
    "\n",
    "# test data\n",
    "y_post = ed.copy(y, {k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta})\n",
    "ty_rep, ty = ed.ppc(lambda xs, zs: tf.reduce_mean(xs[y_post]), \n",
    "       data={y_post: Y_train, \n",
    "            t: X_train['t'],\n",
    "            A: X_train['A'], \n",
    "            X: X_train['X'].as_matrix(), \n",
    "            sigmas: X_train['sigmas'], \n",
    "            t_change: changepoints_t,\n",
    "            tau: changepoint_prior_scale},n_samples=500)               \n",
    "ed.ppc_stat_hist_plot(\n",
    "    ty[0], ty_rep, stat_name=r'$T \\equiv$mean', bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction\n",
    "# sess = ed.get_session()\n",
    "\n",
    "# # TODO: mean?\n",
    "# y_pred = sess.run([y_post.mean()], \n",
    "#                   feed_dict={t: X_test['t'],\n",
    "#                              A: X_test['A'], \n",
    "#                              X: X_test['X'].as_matrix(), \n",
    "#                              sigmas: X_test['sigmas'], \n",
    "#                              t_change: changepoints_t,\n",
    "#                              tau: changepoint_prior_scale})[0]\n",
    "# plt.plot(future['ds'], future['y_scaled'])\n",
    "# plt.plot(future['ds'], y_pred)\n",
    "# plt.show()\n",
    "\n",
    "# # Trend = k + ed.dot(A, delta)) * t + (m + ed.dot(A, gamma)\n",
    "# trend_post = ed.copy(trend_loc, {k: qk, m: qm, delta:qdelta})\n",
    "# seas_post = ed.copy(seas_loc, {beta: qbeta})\n",
    "# trend_pred, seas_pred = sess.run([trend_post, seas_post], \n",
    "#                                  feed_dict={t: X_test['t'],\n",
    "#                              A: X_test['A'], \n",
    "#                              X: X_test['X'].as_matrix(), \n",
    "#                              sigmas: X_test['sigmas'], \n",
    "#                              t_change: changepoints_t,\n",
    "#                              tau: changepoint_prior_scale})\n",
    "\n",
    "# # Plot trend\n",
    "# plt.plot(future['ds'], future['y_scaled'])\n",
    "# plt.plot(future['ds'], trend_pred)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot seasonal\n",
    "# plt.plot(future['ds'], future['y_scaled'])\n",
    "# plt.plot(future['ds'], seas_pred)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot trend + seasonal (no noise)\n",
    "# plt.plot(future['ds'], future['y_scaled'])\n",
    "# plt.plot(future['ds'], trend_pred + seas_pred) # no noise\n",
    "# plt.show()\n",
    "\n",
    "# def smape(y_true, y_pred):\n",
    "#     denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "#     diff = np.abs(y_true - y_pred) / denominator\n",
    "#     diff[denominator == 0] = 0.0\n",
    "#     return 200 * np.mean(diff)\n",
    "\n",
    "# # Metrics \n",
    "# mape = np.mean(np.abs((future['y_scaled'] - y_pred) / future['y_scaled'])) * 100\n",
    "# mse = ((future['y_scaled'] - y_pred) ** 2).mean()\n",
    "# #mse = tf.reduce_mean(tf.square(y_pred - future['y_scaled']))\n",
    "# print(\"MAPE = %f\" % mape)\n",
    "# print(\"SMAPE = %f\" % mape)\n",
    "# print(\"MSE = %f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def visualise(X_data, y_data, k, m, sigma_obs, beta, delta, n_samples=10):\n",
    "#     k = k.sample(n_samples).eval()\n",
    "#     m = m.sample(n_samples).eval()\n",
    "#     sigma_obs = sigma_obs.sample(n_samples).eval()\n",
    "# #     beta = beta.sample(n_samples).eval()\n",
    "# #     delta = delta.sample(n_samples).eval()\n",
    "#     plt.scatter(X_data.iloc[:, 0], y_data)\n",
    "#     inputs = np.linspace(-1, 1, num=400)\n",
    "#     for ns in range(n_samples):\n",
    "#         output = ???\n",
    "#     plt.plot(inputs, output)\n",
    "# visualise(data['X'], data['y'],k, m, sigma_obs, beta, delta)\n",
    "# visualise(data['X'], data['y'],qk, qm, qsigma_obs, qbeta, qdelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from fbprophet import Prophet\n",
    "\n",
    "# # Facebook Prophet\n",
    "# df = pd.read_csv(\"./data/Selena_en.wikipedia.org_all-access_spider.csv\")\n",
    "# df[\"y\"] = np.log(df[\"y\"])\n",
    "# df.head()\n",
    "# m_pp = Prophet()\n",
    "# m_pp.fit(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# future = m_pp.make_future_dataframe(periods=60)\n",
    "# future.tail()\n",
    "# forecast = m_pp.predict(future)\n",
    "# forecast[['ds', 'trend', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "# m_pp.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# m_pp.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evalute(forecast['yhat_'], y_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
