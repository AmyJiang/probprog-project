{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Web Traffic Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from edward.models import Normal, Laplace, Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "FPATH = \"./data/Valentine's_Day_en.wikipedia.org_all-access_spider.csv\"\n",
    "SDATE = pd.datetime(2017, 7, 10)\n",
    "\n",
    "def setup_dataframe(df):\n",
    "    # basic checks and setup\n",
    "    df = df[df['y'].notnull()].copy()\n",
    "    df['y'] = pd.to_numeric(df['y'])\n",
    "    if np.isinf(df['y'].values).any():\n",
    "        raise ValueError(\"Found infinity in column y\")\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    if df['ds'].isnull().any():\n",
    "        raise ValueError(\"Found NaN in column ds\")\n",
    "    df = df.sort_values('ds')\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(FPATH)\n",
    "df[\"y\"] = np.log(df[\"y\"])\n",
    "\n",
    "# Split data into train and test\n",
    "df  = setup_dataframe(df)\n",
    "history = df[df['ds'] <= SDATE].copy()\n",
    "future = df[df['ds'] > SDATE].copy()\n",
    "print(\"History: %d, Future: %d\" % (history.shape[0], future.shape[0]))\n",
    "future.tail()\n",
    "\n",
    "plt.plot(history['ds'],history['y'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def fourier_series(dates, period, order):\n",
    "    # to days since epoch\n",
    "    t = np.array((dates - pd.datetime(1970, 1, 1))\n",
    "                 .dt.total_seconds()\n",
    "                 .astype(np.float)) / (3600 * 24.0)\n",
    "    return np.column_stack([\n",
    "        fun((2.0 * (i + 1) * np.pi * t / period))\n",
    "        for i in range(order)\n",
    "        for fun in (np.sin, np.cos)\n",
    "    ])\n",
    "    \n",
    "def seasonal_feature(dates, period, fourier_order, name):\n",
    "    features = fourier_series(dates, period, fourier_order)\n",
    "    columns = ['{}_delim_{}'.format(name, i + 1) for i in range(features.shape[1])]\n",
    "    return pd.DataFrame(features, columns=columns)\n",
    "    \n",
    "def make_seasonality_features(history, yearly=True, weekly=True, prior_scale=10.0):\n",
    "    start = history['ds'].min()\n",
    "    end = history['ds'].max()\n",
    "    dt = history['ds'].diff()\n",
    "    min_dt = dt.iloc[dt.nonzero()[0]].min() # spacing\n",
    "\n",
    "    seasonal_features = []\n",
    "    prior_scales = []\n",
    "    \n",
    "    # Year seasonality\n",
    "    yearly_disable = end - start < pd.Timedelta(days=730)\n",
    "    if yearly or not yearly_disable:\n",
    "        features = seasonal_feature(history['ds'],\n",
    "                                    period=365.25,\n",
    "                                    fourier_order=10,\n",
    "                                    name='yearly')\n",
    "        seasonal_features.append(features)\n",
    "        prior_scales.extend([prior_scale] * features.shape[1])\n",
    "        \n",
    "    \n",
    "    # Weekly seasonality\n",
    "    weekly_disable = ((end - start < pd.Timedelta(weeks=2)) or\n",
    "                      (min_dt >= pd.Timedelta(weeks=1)))\n",
    "    if weekly or not weekly_disable:\n",
    "        features = seasonal_feature(history['ds'],\n",
    "                                    period=7,\n",
    "                                    fourier_order=3,\n",
    "                                    name='weekly')\n",
    "        seasonal_features.append(features)\n",
    "        prior_scales.extend([prior_scale] * features.shape[1])\n",
    "        \n",
    "    # TODO: holiday\n",
    "    if len(seasonal_features) == 0:\n",
    "        seasonal_features.append(\n",
    "            pd.DataFrame({'zeros': np.zeros(history.shape[0])})\n",
    "        )\n",
    "        prior_scales.append(1.0)\n",
    "    return pd.concat(seasonal_features, axis=1), prior_scales\n",
    "\n",
    "def get_changepoints(history, n_changepoints=25):\n",
    "    # Place potential changepoints evenly through first 80% of history\n",
    "    # Return changepoints_t in t index\n",
    "    \n",
    "    hist_size = np.floor(history.shape[0] * 0.8)\n",
    "    if n_changepoints == -1 or n_changepoints + 1 > hist_size:\n",
    "        n_changepoints = hist_size - 1\n",
    "            \n",
    "    # set changepoints in df['ds'] timestamps\n",
    "    if n_changepoints == 0:\n",
    "        changepoints = [] # no changepoints\n",
    "    else:\n",
    "        cp_indexes = (\n",
    "            np.linspace(0, hist_size, n_changepoints + 1)\n",
    "            .round()\n",
    "            .astype(np.int)\n",
    "        )\n",
    "        changepoints = history.iloc[cp_indexes]['ds'].tail(-1)\n",
    "    \n",
    "    # get changepoints_t in scaled t index\n",
    "    if len(changepoints) > 0:\n",
    "        start = history['ds'].min()\n",
    "        t_scale = history['ds'].max() - start\n",
    "        changepoints_t = np.sort(np.array((changepoints - start) / t_scale))\n",
    "    else:\n",
    "        changepoints_t = np.array([0])  # dummy changepoint\n",
    "    return changepoints_t \n",
    "\n",
    "def get_changepoint_matrix(df, changepoints_t): \n",
    "    A = np.zeros((df.shape[0], len(changepoints_t)))\n",
    "    for i, t_i in enumerate(changepoints_t):\n",
    "        A[df['t'].values >= t_i, i] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a scaled t (time index) and y (#views)\n",
    "t_start = history['ds'].min()\n",
    "t_scale = history['ds'].max() - t_start\n",
    "if t_scale == 0:\n",
    "    raise ValueError(\"Timeseries start == end\")\n",
    "y_scale = history['y'].max()\n",
    "if y_scale == 0:\n",
    "    y_scale = 1\n",
    "history['t'] = (history['ds'] - t_start) / t_scale\n",
    "history['y_scaled'] = history['y'] / y_scale\n",
    "print(\"History dataframe: %d\\n\" % history.shape[0], history.head())\n",
    "\n",
    "# Extract seasonality features\n",
    "seasonal_features, prior_scales = make_seasonality_features(history, yearly=True, weekly=True)\n",
    "print(\"Seasonal features:\\n\")\n",
    "print(seasonal_features.columns)\n",
    "    \n",
    "K = seasonal_features.shape[1] # number of seasonal factors\n",
    "changepoints_t = get_changepoints(history, n_changepoints=25)\n",
    "S = len(changepoints_t) # number of change points\n",
    "changepoint_prior_scale = 0.05\n",
    "\n",
    "print(\"Seasonal_features: %d\\n\" % K)\n",
    "print(\"Changepoints: %d\" % S)\n",
    "\n",
    "X_train = {\n",
    "    't': history['t'].as_matrix(), # day\n",
    "    'A': get_changepoint_matrix(history, changepoints_t), # split indicator\n",
    "    'X': seasonal_features, # seasonal vectors\n",
    "    'sigmas': prior_scales, # scale on seasonality prior\n",
    "}\n",
    "\n",
    "Y_train = history['y_scaled'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "t = tf.placeholder(tf.float32, shape=None, name=\"t\")        # time index\n",
    "A = tf.placeholder(tf.float32, shape=(None, S), name=\"A\")      # changepoint indicators\n",
    "t_change = tf.placeholder(tf.float32, shape=(S), name=\"t_change\") # changepoints_t\n",
    "X = tf.placeholder(tf.float32, shape=(None, K), name=\"X\")      # season vectors\n",
    "sigmas = tf.placeholder(tf.float32, shape=(K,), name=\"sigmas\")  # scale on seasonality prior\n",
    "tau = tf.placeholder(tf.float32, shape=(), name=\"tau\")      # scale on changepoints prior\n",
    "       \n",
    "k = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial slope\n",
    "m = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial intercept\n",
    "sigma_obs = Normal(loc=tf.zeros(1), scale=0.5*tf.ones(1))   # noise\n",
    "\n",
    "delta = Laplace(loc=tf.zeros(S), scale=tau*tf.ones(S))      # changepoint rate adjustment\n",
    "gamma = tf.multiply(-t_change, delta, name=\"gamma\")\n",
    "\n",
    "beta = Normal(loc=tf.zeros(K), scale=sigmas*tf.ones(K))     # seasonal\n",
    "\n",
    "trend_loc = (k + ed.dot(A, delta)) * t + (m + ed.dot(A, gamma))\n",
    "seas_loc = ed.dot(X, beta)\n",
    "y = Normal(loc = trend_loc + seas_loc, scale = sigma_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "ITR = 5000                       # Number of samples.\n",
    "\n",
    "# Init k, m\n",
    "def init_km(df):\n",
    "    i0, i1 = df['ds'].idxmin(), df['ds'].idxmax()\n",
    "    T = df['t'].iloc[i1] - df['t'].iloc[i0]\n",
    "    k = (df['y_scaled'].iloc[i1] - df['y_scaled'].iloc[i0]) / T\n",
    "    m = df['y_scaled'].iloc[i0] -  k * df['t'].iloc[i0]\n",
    "    return (k, m)\n",
    "\n",
    "kinit, minit = init_km(history)\n",
    "print(\"Initial slope / intercept: %f, %f\" % (kinit, minit))\n",
    "qk = Empirical(params=tf.Variable(kinit * tf.ones([ITR, 1])))\n",
    "qm = Empirical(params=tf.Variable(minit * tf.ones([ITR, 1])))\n",
    "qsigma_obs = Empirical(params=tf.Variable(tf.ones([ITR, 1])))\n",
    "qbeta = Empirical(params=tf.Variable(tf.zeros([ITR, K])))\n",
    "qdelta = Empirical(params=tf.Variable(tf.zeros([ITR, S])))\n",
    "\n",
    "inference = ed.HMC({k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta}, \n",
    "                   data={y: Y_train, \n",
    "                         t: X_train['t'],\n",
    "                         A: X_train['A'], \n",
    "                         X: X_train['X'].as_matrix(), \n",
    "                         sigmas: X_train['sigmas'], \n",
    "                         t_change: changepoints_t,\n",
    "                         tau: changepoint_prior_scale})\n",
    "inference.run(step_size=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scaled t and y\n",
    "future['t'] = (future['ds'] - t_start) / t_scale\n",
    "future['y_scaled'] = future['y'] / y_scale\n",
    "print(\"Future dataframe: %d\\n\" % future.shape[0], future.head())\n",
    "print()\n",
    "\n",
    "# Extract seasonality features\n",
    "future_seasonal, future_prior_scales = make_seasonality_features(future, yearly=True, weekly=True)\n",
    "assert(future_seasonal.shape[1] == K)\n",
    "assert(all(future_seasonal.columns == seasonal_features.columns))\n",
    "\n",
    "X_test = {\n",
    "    't': future['t'].as_matrix(), # day\n",
    "    'A': get_changepoint_matrix(future, changepoints_t), # split indicator\n",
    "    'X': future_seasonal, # seasonal vectors\n",
    "    'sigmas': future_prior_scales, # scale on seasonality prior\n",
    "}\n",
    "\n",
    "Y_test = future['y_scaled'].as_matrix()\n",
    "\n",
    "# Evaluate test data\n",
    "y_post = ed.copy(y, {k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta})\n",
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error',  #mean_absolute_percentage_error\n",
    "                  data={y_post: Y_test, \n",
    "                        t: X_test['t'],\n",
    "                        A: X_test['A'], \n",
    "                        X: X_test['X'].as_matrix(), \n",
    "                        sigmas: X_test['sigmas'], \n",
    "                        t_change: changepoints_t,\n",
    "                        tau: changepoint_prior_scale}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get samples of prediction\n",
    "y_pred = np.array([sess.run([y_post], \n",
    "                  feed_dict={t: X_test['t'],\n",
    "                             A: X_test['A'], \n",
    "                             X: X_test['X'].as_matrix(), \n",
    "                             sigmas: X_test['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale}) for _ in range(100)]).mean(axis=0)[0]\n",
    "\n",
    "# Metrics \n",
    "mape = np.mean(np.abs((future['y_scaled'] - y_pred) / future['y_scaled'])) * 100\n",
    "smape = np.mean(np.abs((future['y_scaled'] - y_pred)) / (np.abs((future['y_scaled'] + y_pred)))) * 100\n",
    "mse = ((future['y_scaled'] - y_pred) ** 2).mean()\n",
    "print(\"MAPE = %f\" % mape)\n",
    "print(\"SMAPE = %f\" % smape)\n",
    "print(\"MSE = %f\" % mse)\n",
    "\n",
    "plt.plot(future['ds'],future['y_scaled'])\n",
    "plt.plot(future['ds'],y_pred)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior check\n",
    "sess = ed.get_session()\n",
    "kmean, kstddev = sess.run([qk.mean(), qk.stddev()])\n",
    "print(\"Inferred posterior k: mean = %f, stddev = %f\" % (kmean, kstddev))\n",
    "mmean, mstddev = sess.run([qm.mean(), qm.stddev()])\n",
    "print(\"Inferred posterior m: mean = %f, stddev = %f\" % (mmean, mstddev))\n",
    "\n",
    "noise_mean, noise_stddev = sess.run([qsigma_obs.mean(), qsigma_obs.stddev()])\n",
    "print(\"Inferred posterior noise: mean = %f, stddev = %f\" % (noise_mean, noise_stddev))\n",
    "\n",
    "nburn = 500\n",
    "stride = 10\n",
    "sns.distplot(qk.params.eval()[nburn:ITR:stride])\n",
    "plt.show()\n",
    "sns.distplot(qm.params.eval()[nburn:ITR:stride])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "sess = ed.get_session()\n",
    "\n",
    "# TODO: mean?\n",
    "y_pred = sess.run([y_post.mean()], \n",
    "                  feed_dict={t: X_test['t'],\n",
    "                             A: X_test['A'], \n",
    "                             X: X_test['X'].as_matrix(), \n",
    "                             sigmas: X_test['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale})[0]\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Trend = k + ed.dot(A, delta)) * t + (m + ed.dot(A, gamma)\n",
    "trend_post = ed.copy(trend_loc, {k: qk, m: qm, delta:qdelta})\n",
    "seas_post = ed.copy(seas_loc, {beta: qbeta})\n",
    "trend_pred, seas_pred = sess.run([trend_post, seas_post], \n",
    "                                 feed_dict={t: X_test['t'],\n",
    "                             A: X_test['A'], \n",
    "                             X: X_test['X'].as_matrix(), \n",
    "                             sigmas: X_test['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale})\n",
    "\n",
    "# Plot trend\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], trend_pred)\n",
    "plt.show()\n",
    "\n",
    "# Plot seasonal\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], seas_pred)\n",
    "plt.show()\n",
    "\n",
    "# Plot trend + seasonal (no noise)\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], trend_pred + seas_pred) # no noise\n",
    "plt.show()\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 200 * np.mean(diff)\n",
    "\n",
    "# Metrics \n",
    "mape = np.mean(np.abs((future['y_scaled'] - y_pred) / future['y_scaled'])) * 100\n",
    "# mse = ((future['y_scaled'] - y_pred) ** 2).mean()\n",
    "mse = tf.reduce_mean(tf.square(y_pred - future['y_scaled']))\n",
    "print(\"MAPE = %f\" % mape)\n",
    "print(\"SMAPE = %f\" % mape)\n",
    "print(\"MSE = %f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def visualise(X_data, y_data, k, m, sigma_obs, beta, delta, n_samples=10):\n",
    "#     k = k.sample(n_samples).eval()\n",
    "#     m = m.sample(n_samples).eval()\n",
    "#     sigma_obs = sigma_obs.sample(n_samples).eval()\n",
    "# #     beta = beta.sample(n_samples).eval()\n",
    "# #     delta = delta.sample(n_samples).eval()\n",
    "#     plt.scatter(X_data.iloc[:, 0], y_data)\n",
    "#     inputs = np.linspace(-1, 1, num=400)\n",
    "#     for ns in range(n_samples):\n",
    "#         output = ???\n",
    "#     plt.plot(inputs, output)\n",
    "# visualise(data['X'], data['y'],k, m, sigma_obs, beta, delta)\n",
    "# visualise(data['X'], data['y'],qk, qm, qsigma_obs, qbeta, qdelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def piecewise_linear(t, deltas, k, m, changepoint_ts):\n",
    "    # Intercept changes\n",
    "    gammas = -changepoint_ts * deltas\n",
    "    # Get cumulative slope and intercept at each t\n",
    "    k_t = k * np.ones_like(t)\n",
    "    m_t = m * np.ones_like(t)\n",
    "    for s, t_s in enumerate(changepoint_ts):\n",
    "        indx = t >= t_s\n",
    "        k_t[indx] += deltas[s]\n",
    "        m_t[indx] += gammas[s]\n",
    "    return k_t * t + m_t\n",
    "\n",
    "\n",
    "def add_group_component(components, name, group):\n",
    "    new_comp = components[components['component'].isin(set(group))].copy()\n",
    "    new_comp['component'] = name\n",
    "    components = components.append(new_comp)\n",
    "    return components\n",
    "    \n",
    "def predict_seasonal_components(df, params, data, y_scale, interval_width=0.8):\n",
    "    # TODO: what is interval width?\n",
    "    seasonal_features, _ = make_seasonality_features(df, yearly=True, weekly=True)\n",
    "    lower_p = 100 * (1.0 - interval_width) / 2 \n",
    "    upper_p = 100 * (1.0 + interval_width) / 2\n",
    "    \n",
    "    components = pd.DataFrame({\n",
    "        'col': np.arange(seasonal_features.shape[1]),\n",
    "        'component': [x.split('_delim_')[0] for x in seasonal_features.columns],\n",
    "    })\n",
    "    \n",
    "    # Add a total for seasonal \n",
    "    components = components.append(pd.DataFrame({\n",
    "        'col': np.arange(seasonal_features.shape[1]),\n",
    "        'component': 'seasonal',\n",
    "    }))\n",
    "    \n",
    "    X = seasonal_features.as_matrix()\n",
    "    data = {}\n",
    "    for component, features in components.groupby('component'):\n",
    "        cols = features.col.tolist()\n",
    "        comp_beta = params['beta'][:, cols]\n",
    "        comp_features = X[:, cols]\n",
    "        comp = (np.matmul(comp_features, comp_beta.transpose()) \n",
    "                * y_scale)\n",
    "        data[component] = np.nanmean(comp, axis=1)\n",
    "        data[component + '_lower'] = np.nanpercentile(comp, lower_p, axis=1)\n",
    "        data[component + '_upper'] = np.nanpercentile(comp, upper_p, axis=1)\n",
    "    return pd.DataFrame(data)\n",
    "    \n",
    "\n",
    "def predict(df, history, params, data, nsample=500):\n",
    "    # get posterior predictive mean\n",
    "    k = np.nanmean(params['k'])\n",
    "    m = np.nanmean(params['m'])\n",
    "    deltas = np.nanmean(params['delta'], axis=0)\n",
    "    print(k, m, deltas)\n",
    "    \n",
    "    # predict trend\n",
    "    y_scale = history['y'].max()\n",
    "    df['trend'] = piecewise_linear(np.array(df['t']), deltas, k, m, data[\"t_change\"])\n",
    "    df['trend'] =   df['trend'] * y_scale\n",
    "    \n",
    "    # predict seasonal components \n",
    "    seasonal_components = predict_seasonal_components(df, params, data, y_scale)\n",
    "    #TODO: intervals = predict_uncertainty(df)\n",
    "\n",
    "    df = pd.concat([df, seasonal_components], axis=1)\n",
    "    df['y'] = df['trend'] + df['seasonal']\n",
    "    return df\n",
    "\n",
    "def make_future_dataframe(history, periods, freq='D'):\n",
    "    # create future time series for forecasting\n",
    "    t_start = history['ds'].min()\n",
    "    last_d = history['ds'].max()\n",
    "    t_scale = last_d - t_start\n",
    "    dates = pd.date_range(start=last_d, periods=periods + 1, freq=freq)\n",
    "    dates = dates[dates > last_d]\n",
    "    dates = dates[:periods]\n",
    "    \n",
    "    future = pd.DataFrame({\"ds\": dates})\n",
    "    future['ds'] = pd.to_datetime(future['ds'])\n",
    "    future.reset_index(inplace=True, drop=True)\n",
    "    future['t'] = (future['ds'] - t_start) / t_scale\n",
    "    return future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "future = make_future_dataframe(history, 365)\n",
    "print(\"History:\\n\", history.tail())\n",
    "print()\n",
    "print(\"Future:\\n\",future.head())\n",
    "\n",
    "forecast = predict(future, history, model_params, data)\n",
    "plt.plot(history[\"ds\"], history[\"y\"])\n",
    "plt.plot(forecast[\"ds\"], forecast[\"y\"])\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "# Facebook Prophet\n",
    "df = pd.read_csv(\"./data/Selena_en.wikipedia.org_all-access_spider.csv\")\n",
    "df[\"y\"] = np.log(df[\"y\"])\n",
    "df.head()\n",
    "m_pp = Prophet()\n",
    "m_pp.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "future = m_pp.make_future_dataframe(periods=60)\n",
    "future.tail()\n",
    "forecast = m_pp.predict(future)\n",
    "forecast[['ds', 'trend', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "m_pp.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_pp.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
