{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Web Traffic Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from edward.models import Normal, Laplace, Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/example_wp_peyton_manning.csv\")\n",
    "df[\"y\"] = np.log(df[\"y\"])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "def fourier_series(dates, period, order):\n",
    "    # to days since epoch\n",
    "    t = np.array((dates - pd.datetime(1970, 1, 1))\n",
    "                 .dt.total_seconds()\n",
    "                 .astype(np.float)) / (3600 * 24.0)\n",
    "    return np.column_stack([\n",
    "        fun((2.0 * (i + 1) * np.pi * t / period))\n",
    "        for i in range(order)\n",
    "        for fun in (np.sin, np.cos)\n",
    "    ])\n",
    "    \n",
    "def seasonal_feature(dates, period, fourier_order, name):\n",
    "    features = fourier_series(dates, period, fourier_order)\n",
    "    columns = ['{}_delim_{}'.format(name, i + 1) for i in range(features.shape[1])]\n",
    "    return pd.DataFrame(features, columns=columns)\n",
    "    \n",
    "def make_seasonality_features(history, yearly=True, weekly=True, prior_scale=10.0):\n",
    "    start = history['ds'].min()\n",
    "    end = history['ds'].max()\n",
    "    dt = history['ds'].diff()\n",
    "    min_dt = dt.iloc[dt.nonzero()[0]].min() # spacing\n",
    "\n",
    "    seasonal_features = []\n",
    "    prior_scales = []\n",
    "    \n",
    "    # Year seasonality\n",
    "    yearly_disable = end - start < pd.Timedelta(days=730)\n",
    "    if yearly or not yearly_disable:\n",
    "        features = seasonal_feature(history['ds'],\n",
    "                                    period=365.25,\n",
    "                                    fourier_order=10,\n",
    "                                    name='yearly')\n",
    "        seasonal_features.append(features)\n",
    "        prior_scales.extend([prior_scale] * features.shape[1])\n",
    "        \n",
    "    \n",
    "    # Weekly seasonality\n",
    "    weekly_disable = ((end - start < pd.Timedelta(weeks=2)) or\n",
    "                      (min_dt >= pd.Timedelta(weeks=1)))\n",
    "    if weekly or not weekly_disable:\n",
    "        features = seasonal_feature(history['ds'],\n",
    "                                    period=7,\n",
    "                                    fourier_order=3,\n",
    "                                    name='weekly')\n",
    "        seasonal_features.append(features)\n",
    "        prior_scales.extend([prior_scale] * features.shape[1])\n",
    "        \n",
    "    # TODO: holiday\n",
    "    if len(seasonal_features) == 0:\n",
    "        seasonal_features.append(\n",
    "            pd.DataFrame({'zeros': np.zeros(history.shape[0])})\n",
    "        )\n",
    "        prior_scales.append(1.0)\n",
    "    return pd.concat(seasonal_features, axis=1), prior_scales\n",
    "\n",
    "\n",
    "def get_changepoints(history, n_changepoints=25):\n",
    "    # Place potential changepoints evenly through first 80% of history\n",
    "    # Return changepoints_t in t index\n",
    "    \n",
    "    hist_size = np.floor(history.shape[0] * 0.8)\n",
    "    if n_changepoints == -1 or n_changepoints + 1 > hist_size:\n",
    "        n_changepoints = hist_size - 1\n",
    "            \n",
    "    # set changepoints in df['ds'] timestamps\n",
    "    if n_changepoints == 0:\n",
    "        changepoints = [] # no changepoints\n",
    "    else:\n",
    "        cp_indexes = (\n",
    "            np.linspace(0, hist_size, n_changepoints + 1)\n",
    "            .round()\n",
    "            .astype(np.int)\n",
    "        )\n",
    "        changepoints = history.iloc[cp_indexes]['ds'].tail(-1)\n",
    "    \n",
    "    # set changepoints_t in t index\n",
    "    if len(changepoints) > 0:\n",
    "        start = history['ds'].min()\n",
    "        t_scale = history['ds'].max() - start\n",
    "        changepoints_t = np.sort(np.array((changepoints - start) / t_scale))\n",
    "    else:\n",
    "        changepoints_t = np.array([0])  # dummy changepoint\n",
    "    \n",
    "    # set matrix \n",
    "    A = np.zeros((history.shape[0], len(changepoints_t)))\n",
    "    for i, t_i in enumerate(changepoints_t):\n",
    "        A[history['t'].values >= t_i, i] = 1\n",
    "        \n",
    "    return changepoints_t, A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_dataframe(df):\n",
    "    # Adds a time index t and y_scaled\n",
    "    df = df[df['y'].notnull()].copy()\n",
    "    df['y'] = pd.to_numeric(df['y'])\n",
    "    if np.isinf(df['y'].values).any():\n",
    "        raise ValueError(\"Found infinity in column y\")\n",
    "        \n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "    if df['ds'].isnull().any():\n",
    "        raise ValueError(\"Found NaN in column ds\")\n",
    "    df = df.sort_values('ds')\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Scale ds and y\n",
    "    t_start = df['ds'].min()\n",
    "    t_scale = df['ds'].max() - t_start\n",
    "    if t_scale == 0:\n",
    "        raise ValueError(\"Timeseries start == end\")\n",
    "    y_scale = df['y'].max()\n",
    "    if y_scale == 0:\n",
    "        y_scale = 1\n",
    "    df['t'] = (df['ds'] - t_start) / t_scale\n",
    "    df['y_scaled'] = df['y'] / y_scale\n",
    "    return df\n",
    "\n",
    "# Preprocess data\n",
    "history = setup_dataframe(df)\n",
    "print(\"History dataframe: %d\\n\" % history.shape[0], history.head())\n",
    "print()\n",
    "    \n",
    "# Add seasonality features\n",
    "seasonal_features, prior_scales = make_seasonality_features(history)\n",
    "print(\"Seasonal_features: %d\\n\" % len(prior_scales))\n",
    "print(seasonal_features.columns)\n",
    "print()\n",
    "    \n",
    "# Add changepoints (-1==auto)\n",
    "changepoints_t, A = get_changepoints(history, n_changepoints=25)\n",
    "print(\"Changepoints: %d\" % len(changepoints_t))\n",
    "    \n",
    "changepoint_prior_scale=0.05\n",
    "data = {\n",
    "    'T': history.shape[0], # sampling size\n",
    "    'K': seasonal_features.shape[1], # number of seasonal factors\n",
    "    'S': len(changepoints_t), # number of change points\n",
    "    'y': history['y_scaled'].as_matrix(), # time series\n",
    "    't': history['t'].as_matrix(), # day\n",
    "    'A': A, # split indicator\n",
    "    't_change': changepoints_t, # index of change points\n",
    "    'X': seasonal_features, # seasonal vectors\n",
    "    'sigmas': prior_scales, # scale on seasonality prior\n",
    "    'tau': changepoint_prior_scale, # scale on change point prior\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "T = data['T']\n",
    "S = data['S']\n",
    "K = data['K']\n",
    "\n",
    "t = tf.placeholder(tf.float32, shape=(T,), name=\"t\")        # time index\n",
    "A = tf.placeholder(tf.float32, shape=(T, S), name=\"A\")      # changepoint indicators\n",
    "t_change = tf.placeholder(tf.float32, shape=(S), name=\"t_change\") # changepoints_t\n",
    "X = tf.placeholder(tf.float32, shape=(T, K), name=\"X\")      # season vectors\n",
    "sigmas = tf.placeholder(tf.float32, shape=(K,), name=\"sigmas\")  # scale on seasonality prior\n",
    "tau = tf.placeholder(tf.float32, shape=(), name=\"tau\")      # scale on changepoints prior\n",
    "       \n",
    "k = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial slope\n",
    "m = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial intercept\n",
    "sigma_obs = Normal(loc=tf.zeros(1), scale=0.5*tf.ones(1))   # noise\n",
    "    \n",
    "delta = Laplace(loc=tf.zeros(S), scale=tau*tf.ones(S))      # changepoint rate adjustment\n",
    "gamma = tf.multiply(-t_change, delta, name=\"gamma\")\n",
    "\n",
    "beta = Normal(loc=tf.zeros(K), scale=sigmas*tf.ones(K))     # seasonal\n",
    "    \n",
    "y = Normal(loc = (k * tf.ones(T) + ed.dot(A, delta)) * t \n",
    "           + m * tf.ones(T) + ed.dot(A, gamma)\n",
    "           + ed.dot(X, beta),\n",
    "           scale = sigma_obs * tf.ones(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "ITR = 5000                       # Number of samples.\n",
    "\n",
    "# Init k, m\n",
    "def init_km(df):\n",
    "    i0, i1 = df['ds'].idxmin(), df['ds'].idxmax()\n",
    "    T = df['t'].iloc[i1] - df['t'].iloc[i0]\n",
    "    k = (df['y_scaled'].iloc[i1] - df['y_scaled'].iloc[i0]) / T\n",
    "    m = df['y_scaled'].iloc[i0] -  k * df['t'].iloc[i0]\n",
    "    return (k, m)\n",
    "\n",
    "kinit, minit = init_km(history)\n",
    "print(\"Initial slope / intercept: %f, %f\" % (kinit, minit))\n",
    "qk = Empirical(params=tf.Variable(kinit * tf.ones([ITR, 1])))\n",
    "qm = Empirical(params=tf.Variable(minit * tf.ones([ITR, 1])))\n",
    "qsigma_obs = Empirical(params=tf.Variable(tf.ones([ITR, 1])))\n",
    "qbeta = Empirical(params=tf.Variable(tf.zeros([ITR, K])))\n",
    "qdelta = Empirical(params=tf.Variable(tf.zeros([ITR, S])))\n",
    "\n",
    "inference = ed.HMC({k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta}, \n",
    "                   data={y: data['y'], \n",
    "                         t: data['t'], \n",
    "                         A: data['A'], \n",
    "                         t_change: data['t_change'],\n",
    "                         X: data['X'].as_matrix(), \n",
    "                         sigmas: data['sigmas'], \n",
    "                         tau: data['tau']})\n",
    "inference.run(step_size=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample Posterior\n",
    "sess = ed.get_session()\n",
    "nsample = 500\n",
    "model_params = {\n",
    "    \"k\": sess.run(qk.sample(nsample)), \n",
    "    \"m\": sess.run(qm.sample(nsample)),\n",
    "    \"sigma_obs\": sess.run(qsigma_obs.sample(nsample)),\n",
    "    \"beta\": sess.run(qbeta.sample(nsample)),\n",
    "    \"delta\": sess.run(qdelta.sample(nsample))\n",
    "}\n",
    "print(\"Posterior Mean: k = %f, m = %f\" % (np.nanmean(model_params['k']), \n",
    "                                          np.nanmean(model_params['m'])))\n",
    "print(\"delta = \", np.nanmean(model_params['delta'], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def visualise(X_data, y_data, k, m, sigma_obs, beta, delta, n_samples=10):\n",
    "#     k = k.sample(n_samples).eval()\n",
    "#     m = m.sample(n_samples).eval()\n",
    "#     sigma_obs = sigma_obs.sample(n_samples).eval()\n",
    "# #     beta = beta.sample(n_samples).eval()\n",
    "# #     delta = delta.sample(n_samples).eval()\n",
    "#     plt.scatter(X_data.iloc[:, 0], y_data)\n",
    "#     inputs = np.linspace(-1, 1, num=400)\n",
    "#     for ns in range(n_samples):\n",
    "#         output = ???\n",
    "#     plt.plot(inputs, output)\n",
    "# visualise(data['X'], data['y'],k, m, sigma_obs, beta, delta)\n",
    "# visualise(data['X'], data['y'],qk, qm, qsigma_obs, qbeta, qdelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def piecewise_linear(t, deltas, k, m, changepoint_ts):\n",
    "    # Intercept changes\n",
    "    gammas = -changepoint_ts * deltas\n",
    "    # Get cumulative slope and intercept at each t\n",
    "    k_t = k * np.ones_like(t)\n",
    "    m_t = m * np.ones_like(t)\n",
    "    for s, t_s in enumerate(changepoint_ts):\n",
    "        indx = t >= t_s\n",
    "        k_t[indx] += deltas[s]\n",
    "        m_t[indx] += gammas[s]\n",
    "    return k_t * t + m_t\n",
    "\n",
    "\n",
    "def add_group_component(components, name, group):\n",
    "    new_comp = components[components['component'].isin(set(group))].copy()\n",
    "    new_comp['component'] = name\n",
    "    components = components.append(new_comp)\n",
    "    return components\n",
    "    \n",
    "def predict_seasonal_components(df, params, data, y_scale, interval_width=0.8):\n",
    "    # TODO: what is interval width?\n",
    "    seasonal_features, _ = make_seasonality_features(df, yearly=True, weekly=True)\n",
    "    lower_p = 100 * (1.0 - interval_width) / 2 \n",
    "    upper_p = 100 * (1.0 + interval_width) / 2\n",
    "    \n",
    "    components = pd.DataFrame({\n",
    "        'col': np.arange(seasonal_features.shape[1]),\n",
    "        'component': [x.split('_delim_')[0] for x in seasonal_features.columns],\n",
    "    })\n",
    "    \n",
    "    # Add a total for seasonal \n",
    "    components = components.append(pd.DataFrame({\n",
    "        'col': np.arange(seasonal_features.shape[1]),\n",
    "        'component': 'seasonal',\n",
    "    }))\n",
    "    \n",
    "    X = seasonal_features.as_matrix()\n",
    "    data = {}\n",
    "    for component, features in components.groupby('component'):\n",
    "        cols = features.col.tolist()\n",
    "        comp_beta = params['beta'][:, cols]\n",
    "        comp_features = X[:, cols]\n",
    "        comp = (np.matmul(comp_features, comp_beta.transpose()) \n",
    "                * y_scale)\n",
    "        data[component] = np.nanmean(comp, axis=1)\n",
    "        data[component + '_lower'] = np.nanpercentile(comp, lower_p, axis=1)\n",
    "        data[component + '_upper'] = np.nanpercentile(comp, upper_p, axis=1)\n",
    "    return pd.DataFrame(data)\n",
    "    \n",
    "\n",
    "def predict(df, history, params, data, nsample=500):\n",
    "    # get posterior predictive mean\n",
    "    k = np.nanmean(params['k'])\n",
    "    m = np.nanmean(params['m'])\n",
    "    deltas = np.nanmean(params['delta'], axis=0)\n",
    "    print(k, m, deltas)\n",
    "    \n",
    "    # predict trend\n",
    "    y_scale = history['y'].max()\n",
    "    df['trend'] = piecewise_linear(np.array(df['t']), deltas, k, m, data[\"t_change\"])\n",
    "    df['trend'] =   df['trend'] * y_scale\n",
    "    \n",
    "    # predict seasonal components \n",
    "    seasonal_components = predict_seasonal_components(df, params, data, y_scale)\n",
    "    #TODO: intervals = predict_uncertainty(df)\n",
    "\n",
    "    df = pd.concat([df, seasonal_components], axis=1)\n",
    "    df['y'] = df['trend'] + df['seasonal']\n",
    "    return df\n",
    "\n",
    "def make_future_dataframe(history, periods, freq='D'):\n",
    "    # create future time series for forecasting\n",
    "    t_start = history['ds'].min()\n",
    "    last_d = history['ds'].max()\n",
    "    t_scale = last_d - t_start\n",
    "    dates = pd.date_range(start=last_d, periods=periods + 1, freq=freq)\n",
    "    dates = dates[dates > last_d]\n",
    "    dates = dates[:periods]\n",
    "    \n",
    "    future = pd.DataFrame({\"ds\": dates})\n",
    "    future['ds'] = pd.to_datetime(future['ds'])\n",
    "    future.reset_index(inplace=True, drop=True)\n",
    "    future['t'] = (future['ds'] - t_start) / t_scale\n",
    "    return future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "future = make_future_dataframe(history, 365)\n",
    "print(\"History:\\n\", history.tail())\n",
    "print()\n",
    "print(\"Future:\\n\",future.head())\n",
    "\n",
    "forecast = predict(future, history, model_params, data)\n",
    "plt.plot(history[\"t\"], history[\"y\"])\n",
    "plt.plot(forecast[\"t\"], forecast[\"y\"])\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "# Facebook Prophet\n",
    "df = pd.read_csv(\"./data/example_wp_peyton_manning.csv\")\n",
    "df[\"y\"] = np.log(df[\"y\"])\n",
    "df.head()\n",
    "m_pp = Prophet()\n",
    "m_pp.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m_pp.make_future_dataframe(periods=60)\n",
    "future.tail()\n",
    "forecast = m_pp.predict(future)\n",
    "forecast[['ds', 'trend', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "m_pp.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pp.plot_components(forecast);"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
