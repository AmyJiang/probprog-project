{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Web Traffic Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from edward.models import Normal, Laplace, Empirical\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "FPATH = \"./data/Valentine's_Day_en.wikipedia.org_all-access_spider.csv\"\n",
    "SDATE = pd.datetime(2017, 7, 10)\n",
    "\n",
    "# Prepare dataframe\n",
    "df = pd.read_csv(FPATH)\n",
    "df[\"y\"] = np.log(df[\"y\"])\n",
    "df  = setup_dataframe(df)\n",
    "\n",
    "# Split data into train and test\n",
    "history = df[df['ds'] <= SDATE].copy()\n",
    "future = df[df['ds'] > SDATE].copy()\n",
    "print(\"History: %d, Future: %d\" % (history.shape[0], future.shape[0]))\n",
    "\n",
    "plt.plot(history['ds'],history['y'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Add a scaled t (time index) and y (#views)\n",
    "t_start = history['ds'].min()\n",
    "t_scale = history['ds'].max() - t_start\n",
    "if t_scale == 0:\n",
    "    raise ValueError(\"Timeseries start == end\")\n",
    "y_scale = history['y'].max()\n",
    "if y_scale == 0:\n",
    "    y_scale = 1\n",
    "history['t'] = (history['ds'] - t_start) / t_scale\n",
    "history['y_scaled'] = history['y'] / y_scale\n",
    "print(\"History dataframe: %d\\n\" % history.shape[0], history.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "holiday_en_us = ['2015-01-01', '2015-01-19', '2015-05-25', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', \n",
    "                 '2016-01-01', '2016-01-18', '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-26', \n",
    "                 '2017-01-01', '2017-01-02', '2017-01-16', '2017-05-29', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', \n",
    "                 '2017-12-25',\n",
    "                 '2015-02-14', '2016-02-14', '2017-02-14']\n",
    "holidays = pd.DataFrame({\n",
    "  'holiday': 'US public holiday',\n",
    "  'ds': pd.to_datetime(holiday_en_us),\n",
    "  'lower_window': -1,\n",
    "  'upper_window': 0,\n",
    "  'prior_scale': 50.0\n",
    "})\n",
    "# holidays = None\n",
    "\n",
    "seasonal_features, prior_scales = make_seasonality_features(history, yearly=True, weekly=True, \n",
    "                                                            holidays=holidays)\n",
    "print(\"Seasonal features:\\n\")\n",
    "print(seasonal_features.columns)\n",
    "    \n",
    "K = seasonal_features.shape[1] # number of seasonal factors\n",
    "changepoints_t = get_changepoints(history, n_changepoints=25)\n",
    "S = len(changepoints_t) # number of change points\n",
    "changepoint_prior_scale = 0.05\n",
    "\n",
    "print(\"Seasonal_features: %d\\n\" % K)\n",
    "\n",
    "if holidays is not None:\n",
    "    print(\"Holidays:\\n\")\n",
    "    holiday_ds = []\n",
    "    for feature in seasonal_features:\n",
    "        if feature.split(\"_delim_\")[0] in set(holidays['holiday']):\n",
    "            holiday_ds.extend(seasonal_features[seasonal_features[feature]==1.0].index)\n",
    "    print(history.iloc[np.unique(holiday_ds)][\"ds\"])\n",
    "\n",
    "\n",
    "print(\"Changepoints: %d\" % S)\n",
    "X_train = {\n",
    "    't': history['t'].as_matrix(), # day\n",
    "    'A': get_changepoint_matrix(history, changepoints_t), # split indicator\n",
    "    'X': seasonal_features, # seasonal vectors\n",
    "    'sigmas': prior_scales, # scale on seasonality prior\n",
    "}\n",
    "\n",
    "Y_train = history['y_scaled'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "t = tf.placeholder(tf.float32, shape=None, name=\"t\")        # time index\n",
    "A = tf.placeholder(tf.float32, shape=(None, S), name=\"A\")      # changepoint indicators\n",
    "t_change = tf.placeholder(tf.float32, shape=(S), name=\"t_change\") # changepoints_t\n",
    "X = tf.placeholder(tf.float32, shape=(None, K), name=\"X\")      # season vectors\n",
    "sigmas = tf.placeholder(tf.float32, shape=(K,), name=\"sigmas\")  # scale on seasonality prior\n",
    "tau = tf.placeholder(tf.float32, shape=(), name=\"tau\")      # scale on changepoints prior\n",
    "       \n",
    "k = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial slope\n",
    "m = Normal(loc=tf.zeros(1), scale=5.0*tf.ones(1))           # initial intercept\n",
    "sigma_obs = Normal(loc=tf.zeros(1), scale=0.5*tf.ones(1))   # noise\n",
    "\n",
    "delta = Laplace(loc=tf.zeros(S), scale=tau*tf.ones(S))      # changepoint rate adjustment\n",
    "gamma = tf.multiply(-t_change, delta, name=\"gamma\")\n",
    "\n",
    "beta = Normal(loc=tf.zeros(K), scale=sigmas*tf.ones(K))     # seasonal\n",
    "\n",
    "trend_loc = (k + ed.dot(A, delta)) * t + (m + ed.dot(A, gamma))\n",
    "seas_loc = ed.dot(X, beta)\n",
    "y = Normal(loc = trend_loc + seas_loc, scale = sigma_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "ITR = 5000                       # Number of samples.\n",
    "\n",
    "# Init k, m\n",
    "def init_km(df):\n",
    "    i0, i1 = df['ds'].idxmin(), df['ds'].idxmax()\n",
    "    T = df['t'].iloc[i1] - df['t'].iloc[i0]\n",
    "    k = (df['y_scaled'].iloc[i1] - df['y_scaled'].iloc[i0]) / T\n",
    "    m = df['y_scaled'].iloc[i0] -  k * df['t'].iloc[i0]\n",
    "    return (k, m)\n",
    "\n",
    "kinit, minit = init_km(history)\n",
    "print(\"Initial slope / intercept: %f, %f\" % (kinit, minit))\n",
    "qk = Empirical(params=tf.Variable(kinit * tf.ones([ITR, 1])))\n",
    "qm = Empirical(params=tf.Variable(minit * tf.ones([ITR, 1])))\n",
    "qsigma_obs = Empirical(params=tf.Variable(tf.ones([ITR, 1])))\n",
    "qbeta = Empirical(params=tf.Variable(tf.zeros([ITR, K])))\n",
    "qdelta = Empirical(params=tf.Variable(tf.zeros([ITR, S])))\n",
    "\n",
    "inference = ed.HMC({k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta}, \n",
    "                   data={y: Y_train, \n",
    "                         t: X_train['t'],\n",
    "                         A: X_train['A'], \n",
    "                         X: X_train['X'].as_matrix(), \n",
    "                         sigmas: X_train['sigmas'], \n",
    "                         t_change: changepoints_t,\n",
    "                         tau: changepoint_prior_scale})\n",
    "inference.run(step_size=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scaled t and y\n",
    "future['t'] = (future['ds'] - t_start) / t_scale\n",
    "future['y_scaled'] = future['y'] / y_scale\n",
    "print(\"Future dataframe: %d\\n\" % future.shape[0], future.head())\n",
    "print()\n",
    "\n",
    "# Extract seasonality features\n",
    "future_seasonal, future_prior_scales = make_seasonality_features(future, \n",
    "                                                                 yearly=True, weekly=True,\n",
    "                                                                 holidays=holidays)\n",
    "assert(future_seasonal.shape[1] == K)\n",
    "assert(all(future_seasonal.columns == seasonal_features.columns))\n",
    "\n",
    "X_test = {\n",
    "    't': future['t'].as_matrix(), # day\n",
    "    'A': get_changepoint_matrix(future, changepoints_t), # split indicator\n",
    "    'X': future_seasonal, # seasonal vectors\n",
    "    'sigmas': future_prior_scales, # scale on seasonality prior\n",
    "}\n",
    "\n",
    "Y_test = future['y_scaled'].as_matrix()\n",
    "\n",
    "# Evaluate test data\n",
    "y_post = ed.copy(y, {k: qk, m: qm, sigma_obs: qsigma_obs, beta: qbeta, delta:qdelta})\n",
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error',  #mean_absolute_percentage_error\n",
    "                  data={y_post: Y_test, \n",
    "                        t: X_test['t'],\n",
    "                        A: X_test['A'], \n",
    "                        X: X_test['X'].as_matrix(), \n",
    "                        sigmas: X_test['sigmas'], \n",
    "                        t_change: changepoints_t,\n",
    "                        tau: changepoint_prior_scale}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "sess = ed.get_session()\n",
    "\n",
    "y_pred = np.array([sess.run([y_post], \n",
    "                  feed_dict={t: X_test['t'],\n",
    "                             A: X_test['A'], \n",
    "                             X: X_test['X'].as_matrix(), \n",
    "                             sigmas: X_test['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale}) for _ in range(500)]).mean(axis=0)[0]\n",
    "\n",
    "# Metrics \n",
    "def evalute(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    smape = np.mean(np.abs((y_true - y_pred)) / (np.abs((y_true + y_pred)))) * 100\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    print(\"MAPE = %f\" % mape)\n",
    "    print(\"SMAPE = %f\" % smape)\n",
    "    print(\"MSE = %f\" % mse)\n",
    "    \n",
    "evalute(future['y_scaled'], y_pred)\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], y_pred)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training error \n",
    "y_train_pred = np.array([sess.run([y_post], \n",
    "                  feed_dict={t: X_train['t'],\n",
    "                             A: X_train['A'], \n",
    "                             X: X_train['X'].as_matrix(), \n",
    "                             sigmas: X_train['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale}) for _ in range(500)]).mean(axis=0)[0]\n",
    "\n",
    "# Metrics \n",
    "def evalute(y_true, y_pred):\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    smape = np.mean(np.abs((y_true - y_pred)) / (np.abs((y_true + y_pred)))) * 100\n",
    "    mse = ((y_true - y_pred) ** 2).mean()\n",
    "    print(\"MAPE = %f\" % mape)\n",
    "    print(\"SMAPE = %f\" % smape)\n",
    "    print(\"MSE = %f\" % mse)\n",
    "    \n",
    "evalute(history['y_scaled'], y_train_pred)\n",
    "plt.plot(history['ds'], history['y_scaled'])\n",
    "plt.plot(history['ds'], y_train_pred)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior check\n",
    "kmean, kstddev = sess.run([qk.mean(), qk.stddev()])\n",
    "print(\"Inferred posterior k: mean = %f, stddev = %f\" % (kmean, kstddev))\n",
    "mmean, mstddev = sess.run([qm.mean(), qm.stddev()])\n",
    "print(\"Inferred posterior m: mean = %f, stddev = %f\" % (mmean, mstddev))\n",
    "\n",
    "noise_mean, noise_stddev = sess.run([qsigma_obs.mean(), qsigma_obs.stddev()])\n",
    "print(\"Inferred posterior noise: mean = %f, stddev = %f\" % (noise_mean, noise_stddev))\n",
    "\n",
    "nburn = 500\n",
    "stride = 10\n",
    "sns.distplot(qk.params.eval()[nburn:ITR:stride])\n",
    "plt.show()\n",
    "sns.distplot(qm.params.eval()[nburn:ITR:stride])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "sess = ed.get_session()\n",
    "\n",
    "# TODO: mean?\n",
    "y_pred = sess.run([y_post.mean()], \n",
    "                  feed_dict={t: X_test['t'],\n",
    "                             A: X_test['A'], \n",
    "                             X: X_test['X'].as_matrix(), \n",
    "                             sigmas: X_test['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale})[0]\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Trend = k + ed.dot(A, delta)) * t + (m + ed.dot(A, gamma)\n",
    "trend_post = ed.copy(trend_loc, {k: qk, m: qm, delta:qdelta})\n",
    "seas_post = ed.copy(seas_loc, {beta: qbeta})\n",
    "trend_pred, seas_pred = sess.run([trend_post, seas_post], \n",
    "                                 feed_dict={t: X_test['t'],\n",
    "                             A: X_test['A'], \n",
    "                             X: X_test['X'].as_matrix(), \n",
    "                             sigmas: X_test['sigmas'], \n",
    "                             t_change: changepoints_t,\n",
    "                             tau: changepoint_prior_scale})\n",
    "\n",
    "# Plot trend\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], trend_pred)\n",
    "plt.show()\n",
    "\n",
    "# Plot seasonal\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], seas_pred)\n",
    "plt.show()\n",
    "\n",
    "# Plot trend + seasonal (no noise)\n",
    "plt.plot(future['ds'], future['y_scaled'])\n",
    "plt.plot(future['ds'], trend_pred + seas_pred) # no noise\n",
    "plt.show()\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred))\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 200 * np.mean(diff)\n",
    "\n",
    "# Metrics \n",
    "mape = np.mean(np.abs((future['y_scaled'] - y_pred) / future['y_scaled'])) * 100\n",
    "mse = ((future['y_scaled'] - y_pred) ** 2).mean()\n",
    "#mse = tf.reduce_mean(tf.square(y_pred - future['y_scaled']))\n",
    "print(\"MAPE = %f\" % mape)\n",
    "print(\"SMAPE = %f\" % mape)\n",
    "print(\"MSE = %f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def visualise(X_data, y_data, k, m, sigma_obs, beta, delta, n_samples=10):\n",
    "#     k = k.sample(n_samples).eval()\n",
    "#     m = m.sample(n_samples).eval()\n",
    "#     sigma_obs = sigma_obs.sample(n_samples).eval()\n",
    "# #     beta = beta.sample(n_samples).eval()\n",
    "# #     delta = delta.sample(n_samples).eval()\n",
    "#     plt.scatter(X_data.iloc[:, 0], y_data)\n",
    "#     inputs = np.linspace(-1, 1, num=400)\n",
    "#     for ns in range(n_samples):\n",
    "#         output = ???\n",
    "#     plt.plot(inputs, output)\n",
    "# visualise(data['X'], data['y'],k, m, sigma_obs, beta, delta)\n",
    "# visualise(data['X'], data['y'],qk, qm, qsigma_obs, qbeta, qdelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def piecewise_linear(t, deltas, k, m, changepoint_ts):\n",
    "    # Intercept changes\n",
    "    gammas = -changepoint_ts * deltas\n",
    "    # Get cumulative slope and intercept at each t\n",
    "    k_t = k * np.ones_like(t)\n",
    "    m_t = m * np.ones_like(t)\n",
    "    for s, t_s in enumerate(changepoint_ts):\n",
    "        indx = t >= t_s\n",
    "        k_t[indx] += deltas[s]\n",
    "        m_t[indx] += gammas[s]\n",
    "    return k_t * t + m_t\n",
    "\n",
    "\n",
    "def add_group_component(components, name, group):\n",
    "    new_comp = components[components['component'].isin(set(group))].copy()\n",
    "    new_comp['component'] = name\n",
    "    components = components.append(new_comp)\n",
    "    return components\n",
    "    \n",
    "def predict_seasonal_components(df, params, data, y_scale, interval_width=0.8):\n",
    "    # TODO: what is interval width?\n",
    "    seasonal_features, _ = make_seasonality_features(df, yearly=True, weekly=True)\n",
    "    lower_p = 100 * (1.0 - interval_width) / 2 \n",
    "    upper_p = 100 * (1.0 + interval_width) / 2\n",
    "    \n",
    "    components = pd.DataFrame({\n",
    "        'col': np.arange(seasonal_features.shape[1]),\n",
    "        'component': [x.split('_delim_')[0] for x in seasonal_features.columns],\n",
    "    })\n",
    "    \n",
    "    # Add a total for seasonal \n",
    "    components = components.append(pd.DataFrame({\n",
    "        'col': np.arange(seasonal_features.shape[1]),\n",
    "        'component': 'seasonal',\n",
    "    }))\n",
    "    \n",
    "    X = seasonal_features.as_matrix()\n",
    "    data = {}\n",
    "    for component, features in components.groupby('component'):\n",
    "        cols = features.col.tolist()\n",
    "        comp_beta = params['beta'][:, cols]\n",
    "        comp_features = X[:, cols]\n",
    "        comp = (np.matmul(comp_features, comp_beta.transpose()) \n",
    "                * y_scale)\n",
    "        data[component] = np.nanmean(comp, axis=1)\n",
    "        data[component + '_lower'] = np.nanpercentile(comp, lower_p, axis=1)\n",
    "        data[component + '_upper'] = np.nanpercentile(comp, upper_p, axis=1)\n",
    "    return pd.DataFrame(data)\n",
    "    \n",
    "\n",
    "def predict(df, history, params, data, nsample=500):\n",
    "    # get posterior predictive mean\n",
    "    k = np.nanmean(params['k'])\n",
    "    m = np.nanmean(params['m'])\n",
    "    deltas = np.nanmean(params['delta'], axis=0)\n",
    "    print(k, m, deltas)\n",
    "    \n",
    "    # predict trend\n",
    "    y_scale = history['y'].max()\n",
    "    df['trend'] = piecewise_linear(np.array(df['t']), deltas, k, m, data[\"t_change\"])\n",
    "    df['trend'] =   df['trend'] * y_scale\n",
    "    \n",
    "    # predict seasonal components \n",
    "    seasonal_components = predict_seasonal_components(df, params, data, y_scale)\n",
    "    #TODO: intervals = predict_uncertainty(df)\n",
    "\n",
    "    df = pd.concat([df, seasonal_components], axis=1)\n",
    "    df['y'] = df['trend'] + df['seasonal']\n",
    "    return df\n",
    "\n",
    "def make_future_dataframe(history, periods, freq='D'):\n",
    "    # create future time series for forecasting\n",
    "    t_start = history['ds'].min()\n",
    "    last_d = history['ds'].max()\n",
    "    t_scale = last_d - t_start\n",
    "    dates = pd.date_range(start=last_d, periods=periods + 1, freq=freq)\n",
    "    dates = dates[dates > last_d]\n",
    "    dates = dates[:periods]\n",
    "    \n",
    "    future = pd.DataFrame({\"ds\": dates})\n",
    "    future['ds'] = pd.to_datetime(future['ds'])\n",
    "    future.reset_index(inplace=True, drop=True)\n",
    "    future['t'] = (future['ds'] - t_start) / t_scale\n",
    "    return future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "future = make_future_dataframe(history, 365)\n",
    "print(\"History:\\n\", history.tail())\n",
    "print()\n",
    "print(\"Future:\\n\",future.head())\n",
    "\n",
    "forecast = predict(future, history, model_params, data)\n",
    "plt.plot(history[\"ds\"], history[\"y\"])\n",
    "plt.plot(forecast[\"ds\"], forecast[\"y\"])\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "# Facebook Prophet\n",
    "df = pd.read_csv(\"./data/Selena_en.wikipedia.org_all-access_spider.csv\")\n",
    "df[\"y\"] = np.log(df[\"y\"])\n",
    "df.head()\n",
    "m_pp = Prophet()\n",
    "m_pp.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "future = m_pp.make_future_dataframe(periods=60)\n",
    "future.tail()\n",
    "forecast = m_pp.predict(future)\n",
    "forecast[['ds', 'trend', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
    "m_pp.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_pp.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
